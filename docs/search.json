[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Supplementary Information: Trophic assessment of three sympatric batoid species in the GC",
    "section": "",
    "text": "Code and data"
  },
  {
    "objectID": "index.html#code",
    "href": "index.html#code",
    "title": "Supplementary Information: Trophic assessment of three sympatric batoid species in the GC",
    "section": "Code",
    "text": "Code\nThe code is presented as annotated notebooks, rendered in html files that are in the table of contents of this page.\n\nSupplementary Information 1: ML approach to classify species based on prey counts (Random Forest)\nSupplementary Information 2: Hierarchical Bayesian Model to describe differences in isotopic values between and within species.\nSupplementary Information 3: Amplitudes and comparisons of the isotopic niches (SIBER)\nSupplementary Information 4: Isotopic niche overlaps (nicheROVER)"
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "Supplementary Information: Trophic assessment of three sympatric batoid species in the GC",
    "section": "Data",
    "text": "Data\nThe data is available in the correspondig GitHub repository, along with the unrendered notebooks."
  },
  {
    "objectID": "SI1_ML_counts.html#imports",
    "href": "SI1_ML_counts.html#imports",
    "title": "1  ML approach to classify species based on prey weights",
    "section": "1.1 Imports",
    "text": "1.1 Imports\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport shap\nimport pickle\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score, multilabel_confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom matplotlib import pyplot as plt\n\n\n%load_ext watermark\n%watermark -n -u -v -iv -w -p sklearn\n\nLast updated: Fri Jun 02 2023\n\nPython implementation: CPython\nPython version       : 3.8.6\nIPython version      : 7.19.0\n\nsklearn: 0.0\n\nmatplotlib: 3.3.3\nnumpy     : 1.19.4\npandas    : 1.1.5\nshap      : 0.39.0\nseaborn   : 0.11.0\n\nWatermark: 2.1.0\n\n\n\n\n1.1.1 Miscelaneous configurations\n\n%config InlineBackend.figure_format = 'retina'\nplt.rcParams['font.family'] = 'Serif'\nplt.rcParams['font.size'] = 10\n\noutput_dir = 'output/RandomForest/'"
  },
  {
    "objectID": "SI1_ML_counts.html#data",
    "href": "SI1_ML_counts.html#data",
    "title": "1  ML approach to classify species based on prey weights",
    "section": "1.2 Data",
    "text": "1.2 Data\nReading the data and translating names from Spanish to English:\n\nstomach = pd.read_csv(\"data/stomach_w.csv\")\n\n# Translate column names\ngroup_names = ['Ind', 'Species', 'Season', 'Sex', 'Maturity']\nfeature_names = ['Stomatopods', 'Polychaetes', 'Bivalves', 'Amphipods',\n                 'Crabs', 'Echinoderms', 'Shrimps', 'Crustaceans', 'Sipunculids', 'Fishes']\nstomach.columns = group_names + feature_names\n\n# Translate categories\nstomach.Season = stomach.Season.map({'Fria': 'Cold', 'Calida': 'Warm'})\nstomach.Sex = stomach.Sex.map({'Hembra': 'Female', 'Macho': 'Male'})\nstomach.Maturity = stomach.Maturity.map({'Adulto': 'Adult', 'Juvenil': 'Juvenile'})\n\n# Data preview\nstomach.head()\n\n\n\n\n\n\n\n\nInd\nSpecies\nSeason\nSex\nMaturity\nStomatopods\nPolychaetes\nBivalves\nAmphipods\nCrabs\nEchinoderms\nShrimps\nCrustaceans\nSipunculids\nFishes\n\n\n\n\n0\n1\nH.dipterurus\nCold\nMale\nAdult\n8.50\n3.22\n5.23\n0.00\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n2\nH.dipterurus\nCold\nFemale\nJuvenile\n0.30\n2.02\n0.01\n0.00\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n3\nH.dipterurus\nWarm\nFemale\nJuvenile\n0.50\n0.40\n0.00\n0.00\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n4\nH.dipterurus\nWarm\nMale\nAdult\n0.01\n0.00\n2.86\n0.00\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n5\nH.dipterurus\nCold\nFemale\nJuvenile\n4.54\n2.60\n0.11\n0.03\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\nAssigning weights as an object (X) and species as other (y):\n\n# Just the grouping columns\nstomach_group = stomach.iloc[:, 1:5]\n\n# Weights of the different prey items\nX = stomach.iloc[:, 5:]\n# Target labels\ny = pd.Categorical(stomach_group['Species']).codes"
  },
  {
    "objectID": "SI1_ML_counts.html#model-optimization",
    "href": "SI1_ML_counts.html#model-optimization",
    "title": "1  ML approach to classify species based on prey weights",
    "section": "1.3 Model optimization",
    "text": "1.3 Model optimization\nA Random Forest (RF) classifier was used to model the separation between both species and their categories, given by the weights of each prey group. This approach was preferred over traditional techniques (e.g., PERMANOVA, ANOSIM and/or SIMPER) because it is one of the more efficient machine learning methods and has the following desirable characteristics: a) it is robust to biased distributions, b) it does not require any transformation of the data, c) it is not based on distances or dissimilarities per se, and c) it is resistant to overfitting (Carvajal et al. 2018). The RF approach consists on building t independent decision trees and making an ensemble with them; i.e., the probability of pertenence to each class for every observation is based on the frequency of “votes” made by whole “forest”. The classification is done via a recursive partitioning of the dataset, combining random subsets of both observations and variables (features) of the original data (i.e., bootstraping and bagging) to “learn” the limits between the target classes for each feature.\nEvery ML model should be optimized; i.e., its hyper-parameters should be tuned to maximize its performance. Furthermore, that performance should be always evaluated with data unseen by the model to check for overfitting, regardless of the inherent resistance of the approach. In order to achieve both things, the original data set was divided into a train set and a test set. The train set was used to train the model and tune its hyper-parameters using a grid-search and 5-fold cross-validation: i) number of trees in the ensemble; ii) maximum number of features used per tree, and iii) the maximum “depth” of the tree (i.e., the maximum number of partitions applied to the data). The performance metric used for optimization and evaluation was the Area Under the Curve of the Receiver Operator Characteristics (AUC-ROC), which yields a more reliable evaluation than other metrics, such as the accuracy, since the ROC compares the True Positive Rate and the False Positive Rate at various thresholds (Meyer-Baese & Schmid 2014).\n\n# Train-Test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n\n\n\n\n\n\n\nNote\n\n\n\nThe error of a forest depends on the strength of the individual trees in the forest (i.e., how “good” is each tree) and the correlation between any pair of them (Oshiro et al. 2012). Increasing the correlation increases the forest error rate (i.e., decreases its performance). Our dataset is relatively small, in the sense that it has only 10 features (prey items) and a maximum of 392 instances (total stomachs analyzed from both species) before the train-test split. If we chose a high number of trees, then the feature and instance subsets may be exhausted and continuously repeated during bagging, increasing the correlation between individual trees and, consequently, “hindering” the performance of the forest. In other, more accurate words, increasing the number of trees does not necessarily increase the forest’s performance, and it has been proposed that there is a threshold (dataset dependent) beyond which there is no significant gain (Oshiro et al. 2012).\n\n\n\n# Flag to save outputs\nclassification = 'Global'\n\n# Setting up the optimization via grid-search and cross-validation\nrf_clf = RandomForestClassifier(random_state = 0)\ngrid_values = {'n_estimators': [10, 50, 100, 200, 500],\n               'max_features': list(np.arange(1,10)),\n               'max_depth': list(np.arange(1,50))}\ngrid_rf = GridSearchCV(rf_clf,\n                       param_grid = grid_values,\n                       scoring = 'roc_auc',\n                       n_jobs = 8)\n\n\n%%time\n# Optimization of the model\ngrid_rf.fit(X_train, y_train)\n\nCPU times: user 19.3 s, sys: 2.12 s, total: 21.4 s\nWall time: 6min 25s\n\n\nGridSearchCV(estimator=RandomForestClassifier(random_state=0), n_jobs=8,\n             param_grid={'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n                                       13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n                                       23, 24, 25, 26, 27, 28, 29, 30, ...],\n                         'max_features': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n                         'n_estimators': [10, 50, 100, 200, 500]},\n             scoring='roc_auc')\n\n\n\n#with open(output_dir + f'models/{classification}', 'wb') as model_file:\n#    pickle.dump(grid_rf, model_file)\n\n\n#with open(output_dir + f'models/{classification}', 'rb') as model_file:\n#    grid_rf = pickle.load(model_file)\n\nThe final forest consisted of 200 trees, with a maximum depth of 2 and maximum 3 features per tree:\n\n# Probability of belonging to class 1; i.e., N. entemedor\nypred = grid_rf.predict_proba(X_test)[:,1]\n# Hyper-parameters of the tuned model\nbest_params = grid_rf.best_params_\nbest_params\n\n{'max_depth': 2, 'max_features': 3, 'n_estimators': 200}\n\n\nThe AUC of the test set indicates that the model did not overfit:\n\nprint(f'Training AUC: {round(grid_rf.best_score_, 2)}')\nprint(f'Test AUC: {round(roc_auc_score(y_test, ypred), 2)}' )\n\nTraining AUC: 0.98\nTest AUC: 0.99\n\n\n\n1.3.1 Model training and prey importances\n\n# Fitting of the optimized model\nrf = RandomForestClassifier(max_depth = best_params['max_depth'],\n                            max_features = best_params['max_features'],\n                            n_estimators = best_params['n_estimators'],\n                            n_jobs = 6,\n                            random_state = 0).fit(X_train, y_train)\n\n\n\n1.3.2 SHAP explanations\nFeature importances for a RF classifier are traditionally based on the Gini index. Simply put, they represent how much would the error rate of the model increase, in terms of the purity of the final nodes of the tree, if each feature was removed from the data. On the other hand, SHAP explanations are a more robust and informative approach. These explanations are based on the Shapely values used in coalitional game theory, that is, they connect optimal prediction allocation with local explations based on Shapely values. Shapely values are a method that involves fairly distributing both gains and costs to the actors working in a coalition and, since each actor contributes differently, the Shapely values make shure that each actor gets a fair share depending on their contribution (Lundberg & Lee 2017, Lundberg et al. 2020). In other words, the SHAP approach assigns a higher contribution to the variables that “worked more” for achieving the correct predictions. The main advantages of this approach are that it is model-agnostic (independent of the model) and that it allows both global and local interpretations.\n\n# Function to create a beeswarm and waterfall plot\n# Waterfall plot: barplot showing the relative contributions\n# Beeswarm plot: shows the SHAP values for each observation,\n# colored by the value of the feature\ndef make_shap_beefall_plot(shap_values, features, test, fnames):\n    \n    # Indices of columns\n    column_list = features.columns\n    # Relative contribution\n    feature_ratio = (np.abs(shap_values).sum(0) / np.abs(shap_values).sum()) * 100\n    # Ordering of contributions and indices\n    column_list = column_list[np.argsort(feature_ratio)[::-1]]\n    fratio = np.sort(feature_ratio)[::-1]\n    \n    # Beeswarm plot using SHAP's function\n    shap.summary_plot(shap_values, test,\n                      feature_names = fnames,\n                      show = False,\n                      color_bar_label = 'Weight')\n    # Get current axes\n    ax = plt.gca()\n    # Generate a second X axis\n    ax2 = ax.twiny()\n    # Bar (waterfall) plot\n    ax2.barh(column_list[::-1], fratio[::-1], alpha = 0.3, color = 'gray')\n    # Set ticks\n    ax2.set_xticks(np.linspace(0, round(fratio.max()), 3))\n    # Set axis label\n    ax2.set_xlabel('Relative contribution (%)');\n    # Remove spines\n    sns.despine(left = True, bottom = False, top = False, right = True)\n\n\n# Generate a new \"explainer\" of the model\nexplainer = shap.TreeExplainer(rf)\n# Get the SHAP values for the test data\nshap_values = explainer.shap_values(X_test)\n\n\n1.3.2.1 Global explanation\nThe following beeswarm plot not only shows the feature importance, but also displays the positive and negative relationships of the predictors with the target class (Narcine entemedor). The more positive the SHAP value, the larger the directly proportional correlation of the feature, meaning that high counts of sipunculids correspond to N. entemedor, while low counts are more associated with H. dipterurus. Low counts of bivalves, on the other hand, are positively correlated with N. entemedor and negatively with H. dipterurus. The plot below shows that the main contributior to the differences between both species are mainly due to sipunculids, closely followed by bivalves, explaining ~70% of the differences.\n\nmake_shap_beefall_plot(shap_values[1], # values for class 1; i.e., N. entemedor\n                       features = X_train, # train set for contributions\n                       test = X_test, # test features for beeswarm plot\n                       fnames = feature_names)\n#plt.gcf().set_size_inches(4*1.61,4)\n#plt.savefig(output_dir + f'figures/{classification}/beeswarm_plot.pdf',\n#            bbox_inches = 'tight')\n\n\n\n\nAn easy way to corroborate those results is showing the total counts of sipunculids and bivalves for each species:\n\nstomach.groupby('Species').agg({'Sipunculids': 'sum',\n                                'Bivalves': 'sum'})\n\n\n\n\n\n\n\n\nSipunculids\nBivalves\n\n\nSpecies\n\n\n\n\n\n\nH.dipterurus\n200.16\n340.873\n\n\nN.entemedor\n1437.10\n9.720\n\n\n\n\n\n\n\n\n\n1.3.2.2 Local explanations\nAs for the local interpretations, we can view force plots to explain single observations in the dataset. The 64th individual of the test set had a 1% probability of being N. entemedor (f(x); meaning that it was classified as H. dipterurus) given by the “high” count of bivalves (more correlated with H. dipterurus than N. entemedor):\n\nrename = lambda x: 'H. dipterurus' if x == 0 else 'N. entemedor'\n\n\n#shap.initjs()\n#random_row = np.random.randint(0, 98) # 64\nrandom_row = 64\nf = shap.force_plot(explainer.expected_value[1], shap_values[1][random_row],\n                    X_test.iloc[random_row],\n                    feature_names = feature_names,\n                    matplotlib = True,\n                    show = False)\nplt.title(f'Test ind. # {random_row}; observed: {rename(y_test[random_row])}');\n\n\n\n\n\n#shap.initjs()\n#random_row = np.random.randint(0, 98) # 51\nrandom_row = 51\nf = shap.force_plot(explainer.expected_value[1], shap_values[1][random_row],\n                    X_test.iloc[random_row],\n                    feature_names = feature_names,\n                    matplotlib = True,\n                    show = False)\nplt.title(f'Test ind. #  {random_row}; observed class: {rename(y_test[random_row])}');\n\n\n\n\n\n#shap.initjs()\n#random_row = np.random.randint(0, 98) # 79\nrandom_row = 79\nf = shap.force_plot(explainer.expected_value[1], shap_values[1][random_row],\n                    X_test.iloc[random_row],\n                    feature_names = feature_names,\n                    matplotlib = True,\n                    show = False)\nplt.title(f'Test ind. #  {random_row}; observed class: {rename(y_test[random_row])}');"
  },
  {
    "objectID": "SI1_ML_counts.html#intra-specific-differences",
    "href": "SI1_ML_counts.html#intra-specific-differences",
    "title": "1  ML approach to classify species based on prey weights",
    "section": "1.4 Intra-specific differences",
    "text": "1.4 Intra-specific differences\nUnlike the previous RF in which we tried to discriminate two species, here we will try to discriminate both levels of each covariate within each species.\n\n\n\n\n\n\nImportant\n\n\n\nThere are some dramatic cases of class imbalance. In such instances the results are strongly limited by the small sample sizes of the groups with less observations. Balancing the dataset with subsampling and SMOTE aids only for training and evaluation of the RFs, thus the results should be taken with caution.\n\n\n\n1.4.1 H. dipterurus\n\nstomach_hd = stomach[stomach['Species'] == 'H.dipterurus'].iloc[:, 2:]\nspecies = 'Hd'\n\nThe first step is to check for class imbalances. Only the seasons were balanced (~50% of observations for each class); hence, the other two were pre-processed via undersampling of the majority class and oversampling of the minority class using the Synthetic Majority Oversampling TEchnique. In short, this technique selects examples that are close in the feature space and draws a new sample at a point along that line, allowing to generate as many synthetic observations as required; however, the random undersampling of the majority class is also recommended (Chawla et al. 2002).\n\nseason_n = stomach_hd.groupby('Season').agg({'Season':'count'}).rename(columns = {'Season':'n'})\nseason_n\n\n\n\n\n\n\n\n\nn\n\n\nSeason\n\n\n\n\n\nCold\n109\n\n\nWarm\n96\n\n\n\n\n\n\n\n\nsex_n = stomach_hd.groupby('Sex').agg({'Sex':'count'}).rename(columns = {'Sex':'n'})\nsex_n\n\n\n\n\n\n\n\n\nn\n\n\nSex\n\n\n\n\n\nFemale\n138\n\n\nMale\n67\n\n\n\n\n\n\n\n\nage_n = stomach_hd.groupby('Maturity').agg({'Maturity':'count'}).rename(columns = {'Maturity': 'n'})\nage_n\n\n\n\n\n\n\n\n\nn\n\n\nMaturity\n\n\n\n\n\nAdult\n44\n\n\nJuvenile\n161\n\n\n\n\n\n\n\n\n1.4.1.1 Sexes\n\nclassification = 'Sex'\nsex_n\n\n\n\n\n\n\n\n\nn\n\n\nSex\n\n\n\n\n\nFemale\n138\n\n\nMale\n67\n\n\n\n\n\n\n\nThe sexes were balanced to 100 individuals per class. First, a random undersampling:\n\n# Random undersampling\nunder_female = stomach_hd[stomach_hd.Sex == 'Female'].sample(n = 100, random_state = 0)\nunder_sex = under_female.append(stomach_hd[stomach_hd.Sex == 'Male'])\nunder_sex.Sex = pd.Categorical(under_sex.Sex).codes\n\nThen we apply SMOTE to finish balancing the dataset:\n\nsex_balanced = SMOTE(random_state = 0).fit_resample(under_sex.iloc[:, 3:], under_sex.Sex)\n\nNext we generate the train-test split of the balanced dataset\n\nX_train, X_test, y_train, y_test = train_test_split(sex_balanced[0], sex_balanced[1], random_state = 0)\n\nAnd optimize the Random Forest Classifier\n\nrfm = RandomForestClassifier(random_state = 0)\ngrid_rfm = GridSearchCV(rfm,\n                        param_grid = grid_values,\n                        scoring = 'roc_auc',\n                        n_jobs = 8)\n\n\n%%time\ngrid_rfm.fit(X_train, y_train)\n\nCPU times: user 12.9 s, sys: 1.03 s, total: 13.9 s\nWall time: 6min 10s\n\n\nGridSearchCV(estimator=RandomForestClassifier(random_state=0), n_jobs=8,\n             param_grid={'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n                                       13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n                                       23, 24, 25, 26, 27, 28, 29, 30, ...],\n                         'max_features': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n                         'n_estimators': [10, 50, 100, 200, 500]},\n             scoring='roc_auc')\n\n\n\n#with open(output_dir + f'models/{classification}', 'wb') as model_file:\n#    pickle.dump(grid_rfm, model_file)\n\n\n#with open(output_dir + f'models/{classification}', 'rb') as model_file:\n#    grid_rfm = pickle.load(model_file)\n\nThe forest was optimized with 50 trees, with a maximum depth of 28 and maximum one feature per tree:\n\nbest_params = grid_rfm.best_params_\nbest_params, grid_rfm.best_score_\n\n({'max_depth': 28, 'max_features': 1, 'n_estimators': 50}, 0.7091428571428571)\n\n\n\nrf = RandomForestClassifier(max_depth = best_params['max_depth'],\n                            max_features = best_params['max_features'],\n                            n_estimators = best_params['n_estimators'],\n                            n_jobs = 6,\n                            random_state = 0).fit(X_train, y_train)\n\n\nypred = rf.predict_proba(X_test)[:,1]\n\nThe AUC of the test set indicates that the model did overfit and performed just better than random guessing (AUC = 0.5):\n\nprint(f'Training AUC: {round(grid_rfm.best_score_, 2)}')\nprint(f'Test AUC: {round(roc_auc_score(y_test, ypred), 2)}' )\n\nTraining AUC: 0.71\nTest AUC: 0.6\n\n\n\n1.4.1.1.1 SHAP explanations\n\nexplainer = shap.TreeExplainer(rf)\nshap_values = explainer.shap_values(X_test)\n\n\nmake_shap_beefall_plot(shap_values[1],\n                       features = X_train,\n                       test = X_test,\n                       fnames = feature_names)\nplt.gcf().set_size_inches(4*1.61,4)\nplt.savefig(output_dir + f'figures/Hd/{classification}/beeswarm_plot.pdf',\n            bbox_inches = 'tight')\n\n\n\n\n\n\n\n1.4.1.2 Maturity stages\n\nclassification = 'Maturity'\nage_n\n\n\n\n\n\n\n\n\nn\n\n\nMaturity\n\n\n\n\n\nAdult\n44\n\n\nJuvenile\n161\n\n\n\n\n\n\n\nThe maturity stages were also balanced to 100 individuals per class\n\n# Random undersampling\nunder_juv = stomach_hd[stomach_hd.Maturity == 'Juvenile'].sample(n = 100, random_state = 0)\nunder_age = under_juv.append(stomach_hd[stomach_hd.Maturity == 'Adult'])\nunder_age.Maturity = pd.Categorical(under_age.Maturity).codes\n\n\n# SMOTE balancing\nage_balanced = SMOTE(random_state = 0).fit_resample(under_age.iloc[:, 3:],\n                                                    under_age.Maturity)\n\n\nX_train, X_test, y_train, y_test = train_test_split(age_balanced[0],\n                                                    age_balanced[1],\n                                                    random_state = 0)\n\n\nrfm = RandomForestClassifier(random_state = 0)\ngrid_rfm = GridSearchCV(rfm,\n                        param_grid = grid_values,\n                        scoring = 'roc_auc',\n                        n_jobs = 6)\n\n\n%%time\ngrid_rfm.fit(X_train, y_train)\n\nCPU times: user 9.11 s, sys: 669 ms, total: 9.77 s\nWall time: 6min 55s\n\n\nGridSearchCV(estimator=RandomForestClassifier(random_state=0), n_jobs=6,\n             param_grid={'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n                                       13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n                                       23, 24, 25, 26, 27, 28, 29, 30, ...],\n                         'max_features': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n                         'n_estimators': [10, 50, 100, 200, 500]},\n             scoring='roc_auc')\n\n\n\n#with open(output_dir + f'models/{classification}', 'wb') as model_file:\n#    pickle.dump(grid_rfm, model_file)\n\n\n#with open(output_dir + f'models/{classification}', 'rb') as model_file:\n#    grid_rfm = pickle.load(model_file)\n\n\nbest_params = grid_rfm.best_params_\nbest_params, grid_rfm.best_score_\n\n({'max_depth': 5, 'max_features': 4, 'n_estimators': 50}, 0.6613650793650794)\n\n\n\nrf = RandomForestClassifier(max_depth = best_params['max_depth'],\n                            max_features = best_params['max_features'],\n                            n_estimators = best_params['n_estimators'],\n                            n_jobs = 6,\n                            random_state = 0).fit(X_train, y_train)\n\n\nypred = rf.predict_proba(X_test)[:,1]\n\nThe AUC of the test set indicates that the model did not overfit:\n\nprint(f'Training AUC: {round(grid_rfm.best_score_, 2)}')\nprint(f'Test AUC: {round(roc_auc_score(y_test, ypred), 2)}' )\n\nTraining AUC: 0.66\nTest AUC: 0.6\n\n\n\n1.4.1.2.1 SHAP explanations\n\nexplainer = shap.TreeExplainer(rf)\nshap_values = explainer.shap_values(X_test)\n\n\nmake_shap_beefall_plot(shap_values[1],\n                       features = X_train,\n                       test = X_test,\n                       fnames = feature_names)\nplt.gcf().set_size_inches(4*1.61,4)\nplt.savefig(output_dir + f'figures/Hd/{classification}/beeswarm_plot.pdf',\n            bbox_inches = 'tight')\n\n\n\n\n\n\n\n1.4.1.3 Season\n\nclassification = 'Season'\nseason_n\n\n\n\n\n\n\n\n\nn\n\n\nSeason\n\n\n\n\n\nCold\n109\n\n\nWarm\n96\n\n\n\n\n\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(stomach_hd.iloc[:,3:],\n                                                    stomach_hd.Season, random_state = 0)\n\n\nrfm = RandomForestClassifier(random_state = 0)\ngrid_rfm = GridSearchCV(rfm,\n                        param_grid = grid_values,\n                        scoring = 'roc_auc',\n                        n_jobs = 6)\n\n\n%%time\ngrid_rfm.fit(X_train, y_train)\n\nCPU times: user 9.15 s, sys: 673 ms, total: 9.82 s\nWall time: 6min 57s\n\n\nGridSearchCV(estimator=RandomForestClassifier(random_state=0), n_jobs=6,\n             param_grid={'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n                                       13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n                                       23, 24, 25, 26, 27, 28, 29, 30, ...],\n                         'max_features': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n                         'n_estimators': [10, 50, 100, 200, 500]},\n             scoring='roc_auc')\n\n\n\n#with open(output_dir + f'models/{classification}', 'wb') as model_file:\n#    pickle.dump(grid_rfm, model_file)\n\n\n#with open(output_dir + f'models/{classification}', 'rb') as model_file:\n#    grid_rfm = pickle.load(model_file)\n\n\nbest_params = grid_rfm.best_params_\nbest_params, grid_rfm.best_score_\n\n({'max_depth': 2, 'max_features': 1, 'n_estimators': 100}, 0.6250892857142857)\n\n\n\nrf = RandomForestClassifier(max_depth = best_params['max_depth'],\n                            max_features = best_params['max_features'],\n                            n_estimators = best_params['n_estimators'],\n                            n_jobs = 6,\n                            random_state = 0).fit(X_train, y_train)\n\n\nypred = rf.predict_proba(X_test)[:,1]\n\n\nprint(f'Training AUC: {round(grid_rfm.best_score_, 2)}')\nprint(f'Test AUC: {round(roc_auc_score(y_test, ypred), 2)}' )\n\nTraining AUC: 0.63\nTest AUC: 0.55\n\n\n\n1.4.1.3.1 SHAP explanations\n\nexplainer = shap.TreeExplainer(rf)\nshap_values = explainer.shap_values(X_test)\n\n\nmake_shap_beefall_plot(shap_values[1],\n                       features = X_train,\n                       test = X_test,\n                       fnames = feature_names)\nplt.gcf().set_size_inches(4*1.61,4)\nplt.savefig(output_dir + f'figures/Hd/{classification}/beeswarm_plot.pdf',\n            bbox_inches = 'tight')\n\n\n\n\n\n\n\n\n1.4.2 N. entemedor\n\nspecies = 'Ne'\nstomach_ne = stomach[stomach['Species'] == 'N.entemedor'].iloc[:, 2:]\n\nThe first step is to check for class imbalances. The season variable was the only one balanced (~50% of observations for each class), hence, the other two were pre-processed via undersampling of the overrepresented class and oversampling of the underrepresented class.\n\n1.4.2.1 Sex\n\nclassification = 'Sex'\nsex_n = stomach_ne.groupby('Sex').agg({'Sex':'count'}).rename(columns = {'Sex':'n'})\nsex_n\n\n\n\n\n\n\n\n\nn\n\n\nSex\n\n\n\n\n\nFemale\n154\n\n\nMale\n33\n\n\n\n\n\n\n\nThe sexes were balanced to 100 observations per class\n\n# Random undersampling\nunder_female = stomach_ne[stomach_ne.Sex == 'Female'].sample(n = 100, random_state = 0)\nunder_sex = under_female.append(stomach_ne[stomach_ne.Sex == 'Male'])\nunder_sex.Sex = pd.Categorical(under_sex.Sex).codes\n\n\n# SMOTE balancing\nsex_balanced = SMOTE(random_state = 0).fit_resample(under_sex.iloc[:, 3:], under_sex.Sex)\n\n\nX_train, X_test, y_train, y_test = train_test_split(sex_balanced[0], sex_balanced[1], random_state = 0)\n\n\nrfm = RandomForestClassifier(random_state = 0)\ngrid_rfm = GridSearchCV(rfm,\n                        param_grid = grid_values,\n                        scoring = 'roc_auc',\n                        n_jobs = 6)\n\n\n%%time\ngrid_rfm.fit(X_train, y_train)\n\nCPU times: user 8.83 s, sys: 629 ms, total: 9.46 s\nWall time: 6min 41s\n\n\nGridSearchCV(estimator=RandomForestClassifier(random_state=0), n_jobs=6,\n             param_grid={'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n                                       13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n                                       23, 24, 25, 26, 27, 28, 29, 30, ...],\n                         'max_features': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n                         'n_estimators': [10, 50, 100, 200, 500]},\n             scoring='roc_auc')\n\n\n\n#with open(output_dir + f'models/{classification}', 'wb') as model_file:\n#    pickle.dump(grid_rfm, model_file)\n\n\n#with open(output_dir + f'models/{classification}', 'rb') as model_file:\n#    grid_rfm = pickle.load(model_file)\n\n\nbest_params = grid_rfm.best_params_\nbest_params, grid_rfm.best_score_\n\n({'max_depth': 7, 'max_features': 7, 'n_estimators': 100}, 0.8695496031746032)\n\n\n\nrf = RandomForestClassifier(max_depth = best_params['max_depth'],\n                            max_features = best_params['max_features'],\n                            n_estimators = best_params['n_estimators'],\n                            n_jobs = 6,\n                            random_state = 0).fit(X_train, y_train)\n\n\nypred = rf.predict_proba(X_test)[:,1]\n\nThe AUC of the test set indicates that the model did not overfit:\n\nprint(f'Training AUC: {round(grid_rfm.best_score_, 2)}')\nprint(f'Test AUC: {round(roc_auc_score(y_test, ypred), 2)}')\n\nTraining AUC: 0.87\nTest AUC: 0.87\n\n\n\n1.4.2.1.1 SHAP explanations\n\nexplainer = shap.TreeExplainer(rf)\nshap_values = explainer.shap_values(X_test)\n\n\nmake_shap_beefall_plot(shap_values[1],\n                       features = X_train,\n                       test = X_test,\n                       fnames = feature_names)\n#plt.gcf().set_size_inches(4*1.61,4)\n#plt.savefig(output_dir + f'figures/Ne/{classification}/beeswarm_plot.pdf',\n#            bbox_inches = 'tight')\n\n\n\n\n\n\n\n1.4.2.2 Maturity stages\n\nclassification = 'Maturity'\nage_n = stomach_ne.groupby('Maturity').agg({'Maturity':'count'}).rename(columns = {'Maturity': 'n'})\nage_n\n\n\n\n\n\n\n\n\nn\n\n\nMaturity\n\n\n\n\n\nAdult\n173\n\n\nJuvenile\n14\n\n\n\n\n\n\n\nThe balancing was done to 50 observations per class\n\n# Random undersampling\nunder_juv = stomach_ne[stomach_ne.Maturity == 'Adult'].sample(n = 50, random_state = 0)\nunder_age = under_juv.append(stomach_ne[stomach_ne.Maturity == 'Juvenile'])\nunder_age.Maturity = pd.Categorical(under_age.Maturity).codes\n\n\n# SMOTE balancing\nage_balanced = SMOTE(random_state = 0).fit_resample(under_age.iloc[:, 3:], under_age.Maturity)\n\n\nX_train, X_test, y_train, y_test = train_test_split(age_balanced[0], age_balanced[1], random_state = 0)\n\n\nrfm = RandomForestClassifier(random_state = 0)\ngrid_rfm = GridSearchCV(rfm,\n                        param_grid = grid_values,\n                        scoring = 'roc_auc',\n                        n_jobs = 6)\n\n\n%%time\ngrid_rfm.fit(X_train, y_train)\n\nCPU times: user 8.96 s, sys: 640 ms, total: 9.6 s\nWall time: 6min 37s\n\n\nGridSearchCV(estimator=RandomForestClassifier(random_state=0), n_jobs=6,\n             param_grid={'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n                                       13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n                                       23, 24, 25, 26, 27, 28, 29, 30, ...],\n                         'max_features': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n                         'n_estimators': [10, 50, 100, 200, 500]},\n             scoring='roc_auc')\n\n\n\n#with open(output_dir + f'models/{classification}', 'wb') as model_file:\n#    pickle.dump(grid_rfm, model_file)\n\n\n#with open(output_dir + f'models/{classification}', 'rb') as model_file:\n#    grid_rfm = pickle.load(model_file)\n\n\nbest_params = grid_rfm.best_params_\nbest_params, grid_rfm.best_score_\n\n({'max_depth': 4, 'max_features': 8, 'n_estimators': 100}, 0.7642857142857143)\n\n\n\nrf = RandomForestClassifier(max_depth = best_params['max_depth'],\n                            max_features = best_params['max_features'],\n                            n_estimators = best_params['n_estimators'],\n                            n_jobs = 6,\n                            random_state = 0).fit(X_train, y_train)\n\n\nypred = rf.predict_proba(X_test)[:,1]\n\nThe AUC of the test set indicates that the model did not overfit:\n\nprint(f'Training AUC: {round(grid_rfm.best_score_, 2)}')\nprint(f'Test AUC: {round(roc_auc_score(y_test, ypred), 2)}' )\n\nTraining AUC: 0.76\nTest AUC: 0.83\n\n\n\n1.4.2.2.1 SHAP explanations\n\nexplainer = shap.TreeExplainer(rf)\nshap_values = explainer.shap_values(X_test)\n\n\nmake_shap_beefall_plot(shap_values[1],\n                       features = X_train,\n                       test = X_test,\n                       fnames = feature_names)\n#plt.gcf().set_size_inches(4*1.61,4)\n#plt.savefig(output_dir + f'figures/Ne/{classification}/beeswarm_plot.pdf',\n#            bbox_inches = 'tight')\n\n\n\n\n\n\n\n1.4.2.3 Season\n\nclassification = 'Season'\nseason_n = stomach_ne.groupby('Season').agg({'Season':'count'}).rename(columns = {'Season':'n'})\nseason_n\n\n\n\n\n\n\n\n\nn\n\n\nSeason\n\n\n\n\n\nCold\n99\n\n\nWarm\n88\n\n\n\n\n\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(stomach_ne.iloc[:,3:],\n                                                    stomach_ne.Season, random_state = 0)\n\n\nrfm = RandomForestClassifier(random_state = 0)\ngrid_rfm = GridSearchCV(rfm,\n                        param_grid = grid_values,\n                        scoring = 'roc_auc',\n                        n_jobs = 6)\n\n\n%%time\ngrid_rfm.fit(X_train, y_train)\n\nCPU times: user 8.85 s, sys: 629 ms, total: 9.48 s\nWall time: 6min 44s\n\n\nGridSearchCV(estimator=RandomForestClassifier(random_state=0), n_jobs=6,\n             param_grid={'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n                                       13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n                                       23, 24, 25, 26, 27, 28, 29, 30, ...],\n                         'max_features': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n                         'n_estimators': [10, 50, 100, 200, 500]},\n             scoring='roc_auc')\n\n\n\n#with open(output_dir + f'models/{classification}', 'wb') as model_file:\n#    pickle.dump(grid_rfm, model_file)\n\n\n#with open(output_dir + f'models/{classification}', 'rb') as model_file:\n#    grid_rfm = pickle.load(model_file)\n\n\nbest_params = grid_rfm.best_params_\nbest_params, grid_rfm.best_score_\n\n({'max_depth': 1, 'max_features': 9, 'n_estimators': 50}, 0.6866586538461539)\n\n\n\nrf = RandomForestClassifier(max_depth = best_params['max_depth'],\n                            max_features = best_params['max_features'],\n                            n_estimators = best_params['n_estimators'],\n                            n_jobs = 6,\n                            random_state = 0).fit(X_train, y_train)\n\n\nypred = rf.predict_proba(X_test)[:,1]\n\n\nprint(f'Training AUC: {round(grid_rfm.best_score_, 2)}')\nprint(f'Test AUC: {round(roc_auc_score(y_test, ypred), 2)}' )\n\nTraining AUC: 0.69\nTest AUC: 0.6\n\n\n\n1.4.2.3.1 SHAP explanations\n\nexplainer = shap.TreeExplainer(rf)\nshap_values = explainer.shap_values(X_test)\n\n\nmake_shap_beefall_plot(shap_values[1],\n                       features = X_train,\n                       test = X_test,\n                       fnames = feature_names)\n#plt.gcf().set_size_inches(4*1.61,4)\n#plt.savefig(output_dir + f'figures/Ne/{classification}/beeswarm_plot.pdf',\n#            bbox_inches = 'tight')\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nEnd of notebook\n\n\n\n\n\n\nCarvajal G, Maucec M, Cullick S (2018) Components of artificial intelligence and data analytics. In: Intelligent digital oil and gas fields. Concepts, collaboration, and right-time decisions. Gulf Professional Publishing, Cambridge, Massachusetts, USA, p 101–148\n\n\nChawla NV, Bowyer KW, Hall LO, Kegelmeyer WP (2002) SMOTE: Synthetic Minority Over-sampling Technique. Journal of Artificial Intelligence Research 16:321–357.\n\n\nLundberg S, Lee S-I (2017) A Unified Approach to Interpreting Model Predictions. arXiv preprint 1705.07874.\n\n\nLundberg SM, Erion G, Chen H, DeGrave A, Prutkin JM, Nair B, Katz R, Himmelfarb J, Bansal N, Lee S-I (2020) From Local Explanations to Global Understanding with Explainable AI for Trees. Nature Machine Intelligence 2:56–67.\n\n\nMeyer-Baese A, Schmid V (2014) Chapter 7 - Foundations of neural networks. In: Pattern recognition and signal analysis in medical imaging, 2nd ed. Meyer-Baese A, Schmid V (eds) Academic Press, Oxford, p 197–243\n\n\nOshiro TM, Perez PS, Baranauskas JA (2012) How many trees in a random forest? In: Machine learning and data mining in pattern recognition. Perner P (ed) Springer Berlin Heidelberg, p 154–168"
  },
  {
    "objectID": "SI2_BHGLM.html#imports",
    "href": "SI2_BHGLM.html#imports",
    "title": "2  Hierarchical Bayesian Models to describe differences in isotopic values between and within species",
    "section": "2.1 Imports",
    "text": "2.1 Imports\n\nimport pickle\nimport theano\nimport pandas as pd\nimport numpy as np\nimport pymc3 as pm\nimport arviz as az\nimport seaborn as sns\nimport multiprocessing as mp\nfrom theano import tensor as tt\nfrom matplotlib import pyplot as plt\nfrom threadpoolctl import threadpool_limits\nfrom datetime import datetime\n\n\n2.1.1 Miscelaneous configurations\n\n%env THEANO_FLAGS=device=cpu,floatX=float32\n%env OMP_NUM_THREADS=1\n#%env MKL_NUM_THREADS=1\n%config InlineBackend.figure_format = 'retina'\n\nenv: THEANO_FLAGS=device=cpu,floatX=float32\nenv: OMP_NUM_THREADS=1\n\n\n\nplt.rcParams['font.family'] = 'Serif'\ncolors = plt.rcParams['axes.prop_cycle'].by_key()['color']\nplt.rcParams['font.size'] = 12\noutput_dir = 'output/HBGLM'"
  },
  {
    "objectID": "SI2_BHGLM.html#exploratory-data-analyses",
    "href": "SI2_BHGLM.html#exploratory-data-analyses",
    "title": "2  Hierarchical Bayesian Models to describe differences in isotopic values between and within species",
    "section": "2.2 Exploratory Data Analyses",
    "text": "2.2 Exploratory Data Analyses\nLoading the data from the glm.csv file:\n\n# Loading the data and renaming some columns\ncol_names = {'sexo': 'Sex',\n             'temporada': 'Season',\n             'estadio':'Maturity',\n             'sp': 'Species',\n             'year': 'Year'}\n\n# Drop unused columns and missing data\nglm_data = pd.read_csv('data/glm.csv').rename(columns = col_names).dropna()\n\n# Translating field values to spanish keys\nglm_data.Sex = glm_data.Sex.map({'M':'Male', 'H': 'Female'})\nglm_data.Season = glm_data.Season.map({'Calida': 'Warm', 'Fria': 'Cold'})\nglm_data.Maturity = glm_data.Maturity.map({'Adulto': 'Adult', 'Juvenil': 'Juvenile'})\n\n# Copy of the original dataset, before coding any dummy variables\nref_data = glm_data.copy()\n\n# Preview of the data\nglm_data.head()\n\n\n\n\n\n\n\n\nSpecies\nYear\nSeason\nSex\nMaturity\nd15n\nd13c\n\n\n\n\n0\nH.dipterurus\n2015\nWarm\nMale\nJuvenile\n16.301217\n-14.506785\n\n\n1\nH.dipterurus\n2015\nWarm\nFemale\nJuvenile\n16.832374\n-17.810405\n\n\n2\nH.dipterurus\n2015\nWarm\nMale\nAdult\n17.135463\n-15.831434\n\n\n3\nH.dipterurus\n2015\nWarm\nFemale\nJuvenile\n13.123093\n-15.965960\n\n\n4\nH.dipterurus\n2015\nWarm\nMale\nAdult\n17.633117\n-16.719438\n\n\n\n\n\n\n\n\n2.2.1 Data description\nIn Figure 2.1 we can observe that:\n\nThere is an isotopic gradient between the three species in both isotopes: R. steindachneri < H. dipterurus < N. entemedor.\nThis gradient is consistent, to different extents, through the covariates, causing deviations from the statistic normality, especially in multivariate terms.\nSome classes are misrepresented in relation to anothers\n\n\n# Variable coding:\n# Sex\nn_sex = len(glm_data.Sex.unique()) # Número de categorías (2, evidentemente)\nglm_data['Sex'] = pd.Categorical(glm_data['Sex']).codes # Se transforma a 0 y 1\nsex_idx = glm_data.Sex.values # Se extraen los identificadores\n\n# Season\nn_season = len(glm_data.Season.unique())\nglm_data['Season'] = pd.Categorical(glm_data['Season']).codes\nsex_idx = glm_data.Season.values\n\n# Age\nn_mat = len(glm_data.Maturity.unique())\nglm_data['Maturity'] = pd.Categorical(glm_data['Maturity']).codes\nsex_idx = glm_data.Maturity.values\n\n\n_ = sns.pairplot(glm_data, hue = 'Species', palette = colors[0:3], corner = False)\n\n\n\n\nFigure 2.1: Pairplot of isotopic data and covariates\n\n\n\n\nIt is important to mention that the year variable has two levels only for R. steindachneri and will, hence, be discarded from the analysis. The sample sizes per species-class pair are:\n\nref_data.groupby('Species').agg({'d13c': 'count'})\n\n\n\n\n\n\n\n\nd13c\n\n\nSpecies\n\n\n\n\n\nH.dipterurus\n81\n\n\nN.entemedor\n69\n\n\nR.steindachneri\n74\n\n\n\n\n\n\n\n\nref_data.groupby(['Species', 'Season']).agg({'d13c': 'count'})\n\n\n\n\n\n\n\n\n\nd13c\n\n\nSpecies\nSeason\n\n\n\n\n\nH.dipterurus\nCold\n18\n\n\nWarm\n63\n\n\nN.entemedor\nCold\n15\n\n\nWarm\n54\n\n\nR.steindachneri\nCold\n3\n\n\nWarm\n71\n\n\n\n\n\n\n\n\nref_data.groupby(['Species', 'Sex']).agg({'d13c': 'count'})\n\n\n\n\n\n\n\n\n\nd13c\n\n\nSpecies\nSex\n\n\n\n\n\nH.dipterurus\nFemale\n44\n\n\nMale\n37\n\n\nN.entemedor\nFemale\n55\n\n\nMale\n14\n\n\nR.steindachneri\nFemale\n31\n\n\nMale\n43\n\n\n\n\n\n\n\n\nref_data.groupby(['Species', 'Maturity']).agg({'d13c': 'count'})\n\n\n\n\n\n\n\n\n\nd13c\n\n\nSpecies\nMaturity\n\n\n\n\n\nH.dipterurus\nAdult\n36\n\n\nJuvenile\n45\n\n\nN.entemedor\nAdult\n55\n\n\nJuvenile\n14\n\n\nR.steindachneri\nAdult\n26\n\n\nJuvenile\n48\n\n\n\n\n\n\n\nAnd for every possible combination:\n\nref_data.groupby(['Species', 'Season', 'Sex', 'Maturity']).agg({'d13c':'count'})\n\n\n\n\n\n\n\n\n\n\n\nd13c\n\n\nSpecies\nSeason\nSex\nMaturity\n\n\n\n\n\nH.dipterurus\nCold\nFemale\nAdult\n3\n\n\nJuvenile\n9\n\n\nMale\nAdult\n2\n\n\nJuvenile\n4\n\n\nWarm\nFemale\nAdult\n9\n\n\nJuvenile\n23\n\n\nMale\nAdult\n22\n\n\nJuvenile\n9\n\n\nN.entemedor\nCold\nFemale\nAdult\n13\n\n\nJuvenile\n2\n\n\nWarm\nFemale\nAdult\n30\n\n\nJuvenile\n10\n\n\nMale\nAdult\n12\n\n\nJuvenile\n2\n\n\nR.steindachneri\nCold\nFemale\nJuvenile\n1\n\n\nMale\nAdult\n1\n\n\nJuvenile\n1\n\n\nWarm\nFemale\nAdult\n5\n\n\nJuvenile\n25\n\n\nMale\nAdult\n20\n\n\nJuvenile\n21\n\n\n\n\n\n\n\n\n# Species\nn_sp = len(glm_data.Species.unique())\nsp_names = glm_data['Species'].unique()\nglm_data['Species'] = pd.Categorical(glm_data['Species']).codes\nsp_idx = glm_data.Species.values"
  },
  {
    "objectID": "SI2_BHGLM.html#justification-of-methods",
    "href": "SI2_BHGLM.html#justification-of-methods",
    "title": "2  Hierarchical Bayesian Models to describe differences in isotopic values between and within species",
    "section": "2.3 Justification of methods",
    "text": "2.3 Justification of methods\nFrequentist null hypothesis testing has been useful in ecological studies; however, it has been suggested that inferences should, instead, be made from models, likelihodd ratios or in probabilistic terms (i.e. Bayesian Inference (Ellison 2004, Hobbs & Hilborn 2006, Gerrodette 2011, Armhein et al. 2019)); hence, analyses were based on Bayesian Inference. In general, Bayesian Inference consists on the reallocation of credibility among a space of candidate possibilities, using the Bayes’ Theorem to evaluate the credibility of a parameter given the data and prior knowledge of the parameter (Bolstad 2004, Kruschke 2015). Details on the implementation of the models and the sampling of the posterior distributions are given in each section; however, each model was run until convergence; i.e., 0 divergences during the posterior sampling and Gelman-Rubick statistics equal to 1.0 for every parameter. Other graphical diagnostics such as Posterior Predictive Checks and energy plots (Betancourt 2017, Gabry et al. 2017) are also presented. Every sample after convergence (posterior) was kept. This approach was taken since thinning the posterior sample is unadvised, unless there are computational restrictions, due to a reduction in the precision of the estimates (Link & Eaton 2012, Team 2022). Still, the number of posterior samples for each model was dependent on the number of independent samples (effective sample sizes, ess) being over 2000, both for bulk and tail ess (affecting the reliability of the median and highest density intervals, respectively, Martin 2018)."
  },
  {
    "objectID": "SI2_BHGLM.html#comparisons-of-means-and-effects-of-predictors",
    "href": "SI2_BHGLM.html#comparisons-of-means-and-effects-of-predictors",
    "title": "2  Hierarchical Bayesian Models to describe differences in isotopic values between and within species",
    "section": "2.4 Comparisons of means and effects of predictors ",
    "text": "2.4 Comparisons of means and effects of predictors \nThe isotopic spaces of the three species were described using a custom hierarchichal bivariate model, in which the effects of the climatic seasons (warm vs. cold), sexes and age categories (juveniles vs. adults) on the isotopic values are nested within each species, meaning that the isotopic space of each species is the result of two linear models (one per isotopic ratio) of the covariates. This model was implemented using the pymc3 library (v. 3.11.2, Salvatier et al. 2016) in Python 3 (v. 3.8.2, Van Rossum & Drake 2009), with three chains that were tuned for 25,000 iterations and a posterior sample of 2,000 iterations (draws). The model was specified as follows, where \\(j\\) represents the isotopic ratio, \\(i\\) the species, \\(a\\) the intercept (i.e. the mean isotopic value) and \\(b\\) the slopes of the regression (i.e. the difference between both levels of the covariates):\n\nHyper-priors\n\n\\(\\nu \\sim Exponential(\\lambda = 1/29)+1\\): global degrees-of-freedom (d.f.). The shifted exponential distribution spreads the credibility both for nearly normal and heavy-tailed distributions (Kruschke 2012).\n\\(\\mu_j \\sim StudentT(\\mu = mean(data_j), \\sigma = sd(data_j)*1000, \\nu\\)): Global mean of each isotope, centered on the mean of the pooled data for each isotope and a standard deviation 1000 times that of the pooled data, with the previously described d.f.\n\\(\\sigma_{j,a} \\sim HalfCauchy(\\beta = 10)\\): global standard deviation of the intercepts, following a non-commital Half-Cauchy distribution.\n\\(\\sigma_{j,b} \\sim HalfCauchy(\\beta = 10)\\): global standard deviation of the slopes, with the same distribution as that of the intercepts.\n\\(\\mu_{j,b} \\sim StudentT(\\mu = 0, \\sigma = 10, \\nu)\\): global mean of the slopes, following a non-commital Student-T distribution with the previouslu described d.f.\n\nParameter priors\n\n\\(\\nu_i \\sim Exponential(\\lambda = 1/29) + 1\\): Degrees-of-Freedom for each species, with the same specification that the global d.f.\n\\(Sigma_i \\sim LKJCholesky(n = 2, \\eta = 1, \\sigma = HalfCauchy(\\beta = 10))\\): Covariance matrix for both isotopes for each species, following a Cholesky-decomposed LKJ prior with equi-probability for every matrix, and each standard deviation following a non-commital Half-Cauchy distribution. Used instead of the Wishart distribution due to its higher efficiency to compute when paired with a Cholesky decomposition (Lewandowski et al. 2009).\n\\(\\mu_{j,i} \\sim StudentT(\\mu = \\mu_j, \\sigma_{j,a}, \\nu_i)\\): Intercepts of the linear model for each species and isotopic ratio; i.e., the mean of each isotopic ration with a standard daviation \\(\\sigma_{j,a}\\) and d.f. \\(nu_i\\).\n\\(\\beta_{j,b,i} \\sim Laplace(\\mu_{j,b,i}, b = \\sigma_{j,i})\\): Represents the slope for each isotope and species, following a Laplace distribution, which results in a L1 regularization (i.e. a “Bayesian Lasso” (Park & Casella 2012)), with parameters:\n\n\\(\\mu_{j,b,i} \\sim StudentT(\\mu_{j,b}, \\sigma_{j,b}, \\nu_i)\\): distribution of means of the intercept for each isotopic ratio, following a Student T distribution with mean 0, standard deviation 10 and d.f. \\(\\nu_i\\).\n\\(\\sigma_{n,j,i} \\sim HalfCauchy(\\beta = 10)\\): scale of the Laplace distribution, following a non-commital Half-Caucy distribution.\n\n\nLikelihood model\n\n\\(Y_i \\sim MvStudentT(\\mu = [y_{1,i}, y_{2,i}], \\Sigma_i, \\nu)\\): The bivariate distribution of the isotopic values following a Multivariate Student T distribution, with covariance matrix \\(\\Sigma_i\\), d.f. \\(\\nu\\) and means under the linear model:\n\n\\(y_{j,i} = \\mu_{j,i} + \\beta_{1,j,i}*Sex + \\beta_{2,j,i}*Season + \\beta_{3,j,i}*Age\\)\n\n\n\nThis parametrization obeyed the following reasons: i) a bivariate model allows accounting for the covariation between bulk isotopic values, which is relevant since these depend both on the isotopic baseline and the trophic level; hence, having a joint distribution that is not orthogonal; and, ii) the distributions used have heavier tails than a normal distribution, which assigns a higher probability to extreme values and, thus, allows to make robust estimations of the parameters (Kruschke 2012).\n\n\n\n\n\n\nImportant\n\n\n\nThere are some cases of class imbalance and small sample sizes. Hierarchical Bayesian models aid in propagating the uncertainty and make every sample “contribute” to every estimate (including those of other groups) due to the partial pooling of the data and the effect of shrinkage, minimizing the effect of small sample sizes. This effect is further minimized by the robust inference allowed by the Student t likelihood. Moreover, SIA integrates information over a time window, “reducing” (to some extent) the sample size needed to make inferences (Jackson et al. 2011). Nevertheless, neither Bayesian inference nor SIA can make information out of thin air, and the results for those groups should be carefuly interpreted.\n\n\n\nwith pm.Model() as full_model:\n    \n    obs = glm_data[['d13c', 'd15n']].values\n    \n    #----Hyper-priors----\n    ## Degrees of freedom.\n    # Shifted exponential to spread the credibility among heavy-tailed\n    # (small d.f.) and light-tailed (d.f) distributions (normal-like)\n    nu = pm.Exponential('ν', lam = 1/29) + 1\n    \n    ## Mean Isotopic values ~ StudentT (Gaussian-like, heavier tails than a normal dist.)\n    # Centered on the global mean of each isotope to keep the distribution scaled\n    # Standard deviation: 1000 times the standard deviation of the pooled data\n    # D.F.: Previously defined\n    \n    µ1 = pm.StudentT('δ13C', mu = glm_data.d13c.mean(),\n                     sd = glm_data.d13c.std()*1000,\n                     nu = nu)\n    µ2 = pm.StudentT('δ15N', mu = glm_data.d15n.mean(),\n                     sd = glm_data.d15n.std()*1000,\n                     nu = nu)    \n    ## Parameters\n    # Standard deviation of the intercepts. Non-commital Half-Cauchy.\n    sigma_a = pm.HalfCauchy('sd_a', beta = 10)\n    \n    # Standard deviation of the slopes. Non-commital Half-Cauchy.\n    sigma_b = pm.HalfCauchy('sd_b', beta = 10)\n    \n    # D.F. of the Distribution. Same parametrization as before.\n    nu2 = pm.Exponential('nu_a', lam = 1/29) + 1\n    \n    # Distribution of means of the slopes. Non-commital Student-T distribution.\n    mu_b = pm.StudentT('mu_b', mu = 0, sd = 10, nu = nu)\n    \n    #----Priors on the parameters----\n    ## Means: Student-T\n    mu1 = pm.StudentT('µδ13C',\n                      mu = µ1,\n                      sigma = pm.HalfCauchy('sδ13C', beta = 10),\n                      nu = nu2,\n                      shape = n_sp)\n    mu2 = pm.StudentT('µδ15N',\n                      mu = µ2,\n                      sigma = pm.HalfCauchy('sδ15N', beta = 10),\n                      nu = nu2,\n                      shape = n_sp)\n    \n    ## Intercepts: Student-T distribution centered on global mean isotopic values\n    a1 = pm.StudentT('⍺δ13C', mu = µ1, sigma = sigma_a, nu = nu2, shape = n_sp)\n    a2 = pm.StudentT('⍺δ15N', mu = µ2, sigma = sigma_a, nu = nu2, shape = n_sp)\n    \n    ## Slopes: Student T distribution   \n\n    ## Slopes: Laplace distribution. \n    # Equivalent to a Lasso regression; i.e., L1 regularization.\n    # Affects the sum of absolutes of the residuals. The effect is that small effects -> 0.\n    # Especially useful when considering the measuring error of the mass spectrometer.\n    \n    # Main effects:\n    # Sex:\n    T_b1c = pm.Laplace('Sex_δ13C', mu = mu_b, b = sigma_b, shape = n_sp)\n    T_b1n = pm.Laplace('Sex_δ15N', mu = mu_b, b = sigma_b, shape = n_sp)\n    # Season:\n    T_b2c = pm.Laplace('Season_δ13C', mu = mu_b, b = sigma_b, shape = n_sp)\n    T_b2n = pm.Laplace('Season_δ15N', mu = mu_b, b = sigma_b, shape = n_sp)\n    # Maturity stages:\n    T_b3c = pm.Laplace('Maturity_δ13C', mu = mu_b, b = sigma_b, shape = n_sp)\n    T_b3n = pm.Laplace('Maturity_δ15N', mu = mu_b, b = sigma_b, shape = n_sp)\n    \n    \n    ## Covariance Matrix\n    # LKJ prior with eta = 1, meaning a uniform distribution of the whole matrix\n    # Prior of every sd: Non-commital Half-Cauchy\n    pL = pm.LKJCholeskyCov('pL', n = 2, eta = 1, \n                           sd_dist = pm.HalfCauchy.dist(beta = 10), shape = n_sp)\n    # The LKJ distribution is decomposed by a Cholesky factor, so we decompose it:\n    L = pm.expand_packed_triangular(2, pL)\n    Σ = pm.Deterministic('Σ', L.dot(L.T))\n    \n    #----Algebraic expression of the model----\n    # Each isotope is a linear model with the effect of the covariates\n    d13c = a1[sp_idx] +\\\n           T_b1c[sp_idx]*glm_data.Sex.values +\\\n           T_b2c[sp_idx]*glm_data.Season.values +\\\n           T_b3c[sp_idx]*glm_data.Maturity.values\n    d15n = a2[sp_idx] +\\\n           T_b1n[sp_idx]*glm_data.Sex.values +\\\n           T_b2n[sp_idx]*glm_data.Season.values +\\\n           T_b3n[sp_idx]*glm_data.Maturity.values\n    \n    # Bivariate tensor for the Likelihood model:\n    glm = tt.stack([d13c, d15n]).T\n    mus = tt.stack([mu1[sp_idx], mu2[sp_idx]]).T\n    \n    #----Likelihood model----\n    # In this step the hierarchical estimates are included;\n    # hence, it includes their uncertainty.\n    y = pm.MvStudentT('mvT_sp',\n                      mu = glm,# Linear model of each isotopes\n                      cov = Σ, # Covariance Matrix\n                      nu = nu, # Degrees of freedom\n                      observed = obs) # Observed data\n    \n    means = pm.MvStudentT('µ_sp',\n                          mu = mus,\n                          cov = Σ,\n                          nu = nu,\n                          observed = obs)\n\nGraph of the hierarchical model.\n\npm.model_to_graphviz(full_model)\n\n\n\n\nFigure 2.2: Directed graph of the hierarchical model.\n\n\n\n\n\n# Parameters of the NUTS.\ntune = 25000\ndraws = 5000\n# Variational Inference to start the sampling process\ninit = 'advi+adapt_diag'\n\nSampling of the posterior:\n\nwith full_model:\n    #----Sampling----\n    full_trace = pm.sample(draws = draws, # Posterior samples to keep\n                           tune = tune, # Burn-in iterations\n                           chains = 3, # Number of chains\n                           cores = 3, # Number of chains run in parallel \n                           init = init, # Initiation method,\n                           return_inferencedata = False, # NOT return an arviz.InferenceData\n                           random_seed = 0, # For consistency\n                           progressbar = False) # NOT show a progress bar\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using advi+adapt_diag...\nConvergence achieved at 19700\nInterrupted at 19,699 [9%]: Average Loss = 1,722.9\nMultiprocess sampling (3 chains in 3 jobs)\nNUTS: [pL, Maturity_δ15N, Maturity_δ13C, Season_δ15N, Season_δ13C, Sex_δ15N, Sex_δ13C, ⍺δ15N, ⍺δ13C, µδ15N, sδ15N, µδ13C, sδ13C, mu_b, nu_a, sd_b, sd_a, δ15N, δ13C, ν]\nSampling 3 chains for 25_000 tune and 5_000 draw iterations (75_000 + 15_000 draws total) took 1038 seconds.\n\n\n\n2.4.1 Diagnostics \n\nfull_mod = az.from_pymc3(trace = full_trace, model = full_model)\n\n\n# Save model\n#with open(output_dir+'/model/az_model', 'wb') as model_file:\n#    pickle.dump(full_mod, model_file)\n# Save trace\n#with open(output_dir+'/model/pm_trace', 'wb') as model_file:\n#    pickle.dump(full_trace, model_file)\n\n\n# Load model for consistency\n#with open(output_dir+'/model/az_model', 'rb') as model_file:\n#    full_mod = pickle.load(model_file)\n#with open(output_dir+'/model/az_model', 'rb') as model_file:\n#    full_trace = pickle.load(model_file)\n\nSummary statistics of the posterior distributions of the parameters. Gelman-Statistic values equal to 1.0 (< 1.01) and Effective Sample Sizes (both Bulk and Tail) over 2000 for every parameter.\n\naz.summary(full_mod, hdi_prob = 0.95)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_2.5%\nhdi_97.5%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nδ13C\n-13.995\n0.904\n-15.885\n-12.218\n0.009\n0.006\n12202.0\n8109.0\n1.0\n\n\nδ15N\n16.508\n0.764\n14.956\n17.988\n0.008\n0.005\n11778.0\n7825.0\n1.0\n\n\nmu_b\n-0.111\n0.145\n-0.405\n0.172\n0.001\n0.001\n14501.0\n9645.0\n1.0\n\n\nµδ13C[0]\n-13.997\n0.191\n-14.370\n-13.624\n0.002\n0.001\n12398.0\n11332.0\n1.0\n\n\nµδ13C[1]\n-12.385\n0.129\n-12.630\n-12.125\n0.001\n0.001\n15640.0\n11048.0\n1.0\n\n\nµδ13C[2]\n-16.119\n0.109\n-16.338\n-15.910\n0.001\n0.001\n13528.0\n11247.0\n1.0\n\n\nµδ15N[0]\n16.161\n0.149\n15.869\n16.452\n0.001\n0.001\n12816.0\n11541.0\n1.0\n\n\nµδ15N[1]\n18.058\n0.099\n17.870\n18.255\n0.001\n0.001\n14475.0\n11300.0\n1.0\n\n\nµδ15N[2]\n16.179\n0.087\n16.006\n16.344\n0.001\n0.001\n12963.0\n11430.0\n1.0\n\n\n⍺δ13C[0]\n-14.028\n0.480\n-14.927\n-13.065\n0.006\n0.004\n7143.0\n9346.0\n1.0\n\n\n⍺δ13C[1]\n-12.158\n0.227\n-12.608\n-11.715\n0.002\n0.002\n9716.0\n9840.0\n1.0\n\n\n⍺δ13C[2]\n-15.529\n0.483\n-16.436\n-14.538\n0.006\n0.004\n6458.0\n7192.0\n1.0\n\n\n⍺δ15N[0]\n15.231\n0.359\n14.513\n15.925\n0.004\n0.003\n6816.0\n9450.0\n1.0\n\n\n⍺δ15N[1]\n17.788\n0.200\n17.400\n18.180\n0.002\n0.002\n8775.0\n9503.0\n1.0\n\n\n⍺δ15N[2]\n15.752\n0.534\n14.645\n16.676\n0.007\n0.005\n6350.0\n8275.0\n1.0\n\n\nSex_δ13C[0]\n-0.302\n0.272\n-0.855\n0.219\n0.002\n0.002\n13199.0\n10522.0\n1.0\n\n\nSex_δ13C[1]\n-0.134\n0.272\n-0.694\n0.395\n0.002\n0.002\n15605.0\n10292.0\n1.0\n\n\nSex_δ13C[2]\n0.060\n0.196\n-0.315\n0.456\n0.002\n0.001\n14920.0\n11701.0\n1.0\n\n\nSex_δ15N[0]\n-0.082\n0.207\n-0.472\n0.341\n0.002\n0.001\n11893.0\n10560.0\n1.0\n\n\nSex_δ15N[1]\n-0.190\n0.220\n-0.635\n0.238\n0.002\n0.002\n15431.0\n10737.0\n1.0\n\n\nSex_δ15N[2]\n-0.024\n0.157\n-0.333\n0.286\n0.001\n0.001\n16323.0\n11103.0\n1.0\n\n\nSeason_δ13C[0]\n-0.728\n0.371\n-1.434\n-0.017\n0.004\n0.003\n9062.0\n10988.0\n1.0\n\n\nSeason_δ13C[1]\n-0.203\n0.254\n-0.725\n0.275\n0.003\n0.002\n9764.0\n9525.0\n1.0\n\n\nSeason_δ13C[2]\n-0.304\n0.435\n-1.218\n0.529\n0.005\n0.004\n7016.0\n7164.0\n1.0\n\n\nSeason_δ15N[0]\n1.784\n0.297\n1.206\n2.366\n0.003\n0.002\n9305.0\n10307.0\n1.0\n\n\nSeason_δ15N[1]\n0.458\n0.229\n0.022\n0.922\n0.002\n0.002\n8542.0\n8725.0\n1.0\n\n\nSeason_δ15N[2]\n0.414\n0.506\n-0.441\n1.468\n0.006\n0.005\n6719.0\n8149.0\n1.0\n\n\nMaturity_δ13C[0]\n1.157\n0.398\n0.378\n1.934\n0.004\n0.003\n8695.0\n10915.0\n1.0\n\n\nMaturity_δ13C[1]\n-0.245\n0.294\n-0.841\n0.329\n0.003\n0.002\n13819.0\n10579.0\n1.0\n\n\nMaturity_δ13C[2]\n-0.463\n0.218\n-0.898\n-0.053\n0.002\n0.001\n13527.0\n11509.0\n1.0\n\n\nMaturity_δ15N[0]\n-0.897\n0.279\n-1.445\n-0.350\n0.003\n0.002\n8263.0\n11065.0\n1.0\n\n\nMaturity_δ15N[1]\n-0.389\n0.238\n-0.866\n0.064\n0.002\n0.002\n15017.0\n10699.0\n1.0\n\n\nMaturity_δ15N[2]\n0.022\n0.170\n-0.298\n0.376\n0.001\n0.001\n13985.0\n11095.0\n1.0\n\n\nν\n1.559\n0.340\n0.924\n2.229\n0.003\n0.002\n9236.0\n8727.0\n1.0\n\n\nsd_a\n1.892\n0.877\n0.681\n3.529\n0.009\n0.007\n14428.0\n9423.0\n1.0\n\n\nsd_b\n0.546\n0.168\n0.274\n0.884\n0.002\n0.001\n8818.0\n9953.0\n1.0\n\n\nnu_a\n32.802\n30.069\n0.097\n93.053\n0.223\n0.178\n14556.0\n8534.0\n1.0\n\n\nsδ13C\n2.936\n2.130\n0.644\n6.783\n0.023\n0.016\n14702.0\n8659.0\n1.0\n\n\nsδ15N\n2.060\n1.847\n0.303\n5.230\n0.021\n0.015\n11879.0\n7494.0\n1.0\n\n\npL[0]\n0.944\n0.059\n0.833\n1.064\n0.001\n0.000\n8645.0\n9254.0\n1.0\n\n\npL[1]\n-0.471\n0.044\n-0.554\n-0.381\n0.000\n0.000\n11209.0\n11333.0\n1.0\n\n\npL[2]\n0.560\n0.035\n0.492\n0.628\n0.000\n0.000\n9231.0\n9005.0\n1.0\n\n\nΣ[0,0]\n0.895\n0.112\n0.680\n1.117\n0.001\n0.001\n8645.0\n9254.0\n1.0\n\n\nΣ[0,1]\n-0.446\n0.064\n-0.573\n-0.324\n0.001\n0.000\n9200.0\n9280.0\n1.0\n\n\nΣ[1,0]\n-0.446\n0.064\n-0.573\n-0.324\n0.001\n0.000\n9200.0\n9280.0\n1.0\n\n\nΣ[1,1]\n0.538\n0.068\n0.405\n0.670\n0.001\n0.001\n7994.0\n8217.0\n1.0\n\n\n\n\n\n\n\n\npps = pm.sample_posterior_predictive(trace = full_trace,\n                                     model = full_model,\n                                     progressbar = False,\n                                     random_seed = 0)\nposteriors = az.from_pymc3(posterior_predictive = pps,\n                           model = full_model)\n\n\n# Save posterior predictive samples\n#with open(output_dir+'/model/posteriors_mvt', 'wb') as post_mvt_file:\n#    pickle.dump(posteriors, post_mvt_file)\n\n\n# Load posterior predictive samples\n#with open(output_dir+'/model/posteriors_mvt', 'rb') as post_mvt_file:\n#    posteriors = pickle.load(post_mvt_file)\n\nPosterior Predictive Checks for the likelihood model. The observed distribution (black) is between the posterior predictive samples (blue). Each posterior predictive sample consists on a set with the same number of observations as the original data, generated based on a parameter from the posterior samples.\n\naz.plot_ppc(posteriors, mean = False, kind = 'cumulative', num_pp_samples = 500);\n\n\n\n\nFigure 2.3: Posterior Predictive Check using the CDF.\n\n\n\n\n\naz.plot_ppc(posteriors, kind = 'scatter', mean = False);\n\n\n\n\nFigure 2.4: Posterior Predictive Check using scatterplots.\n\n\n\n\nEnergy plot and Bayesian Fraction of Missing Information (BFMI). The energy plot shows similar densities between the marginal and transition energies from the Hamiltonian simulation, meaning that the sampler was not stuck in a particular area of the posterior distribution, which is validated through the BFMI values (Betancourt 2017).\n\naz.plot_energy(full_mod, fill_color = ('C0', 'C3'));\n\n\n\n\nFigure 2.5: Energy plot and Bayesian Fraction of Missing Information.\n\n\n\n\n\n2.4.1.1 Effective sample sizes, autocorrelations and no-thinning.\nThe autocorrelation (i.e., the correlation between the sample\\(_i\\) and the sample\\(_{i-1}\\)) directly affects the effective sample size (ess) estimation (higher autocorrelations, lower ess). For this reason, a very common practice is to thin the posterior sample (i.e. keep only 1 out of every k-th iteration after convergence); however, this is unadvised since it leads to a reduction in the precision of the estimates(Link & Eaton 2012), making it feasible only to reduce computational requirements (Team 2022). Moreover, one of the advantages of using NUTS is that the samples can be uncorrelated, since the new state of the chain depends more on the probability “surface” rather than the previous position of the particle. Shown in the following autocorrelation plot:\n\nrc = {'plot.max_subplots': 120}\naz.rcParams.update(rc)\naz.plot_autocorr(full_mod, combined = True);\n\n\n\n\nFigure 2.6: Plots of autocorrelations of the posterior sample.\n\n\n\n\nParallel coordinates plots for each parameter of interest within the model. Shows that every sample of the posterior distribution was non-divergent. A) Mean values for each species and B) Slopes for each covariate for each species; where: 0: H. dipterurus, 1: N. entemedor, 2: R. steindachneri\n\naz.plot_parallel(full_mod, var_names=['µ'], filter_vars='like');\n\n\n\n\nFigure 2.7: Parallel coordinates plot for the means of the isotopic values.\n\n\n\n\n\nax = az.plot_parallel(full_mod, var_names = ['Season', 'Sex', 'Maturity'], filter_vars = 'like');\nax.set_xticklabels(ax.get_xticklabels(), rotation=70);\n\n\n\n\nFigure 2.8: Parallel coordinates plot for the slopes of the covariates."
  },
  {
    "objectID": "SI2_BHGLM.html#a-note-on-the-interpretation-of-results",
    "href": "SI2_BHGLM.html#a-note-on-the-interpretation-of-results",
    "title": "2  Hierarchical Bayesian Models to describe differences in isotopic values between and within species",
    "section": "2.5 A note on the interpretation of results ",
    "text": "2.5 A note on the interpretation of results \nIt is important to understand the interpretation of the slope of a categorical covariate before going to the description of the results. Consider a univariate model with a binary covariate (\\(X\\)) such as the ones of this model:\n\\(Y = \\beta_0 + \\beta_1*X + \\epsilon\\)\n\\(\\therefore\\)\nIf \\(X = 0\\): \\(Y_0 = \\beta_0 + \\beta_1*0 + \\epsilon \\implies Y_0 = \\beta_0 + \\epsilon\\)\nIf \\(X = 1\\): \\(Y_0 = \\beta_0 + \\beta_1*1 + \\epsilon \\implies Y_1 = \\beta_0 + \\beta_1 + \\epsilon\\)\nI.e., the slope represents the difference between the means of both classes. This is demonstrabe if we substract both equations (\\(\\Delta_Y\\)):\n\\(Y = \\beta_0 + \\beta_1 + \\epsilon = \\beta_0 + \\epsilon \\implies \\Delta_Y = \\beta_0 - \\beta_0 + \\beta_1 + \\epsilon - \\epsilon \\therefore \\Delta_Y = \\beta_1\\)\nTaking this into account, the slopes in this work represent the difference (in ‰) between the two classes being analysed."
  },
  {
    "objectID": "SI2_BHGLM.html#results",
    "href": "SI2_BHGLM.html#results",
    "title": "2  Hierarchical Bayesian Models to describe differences in isotopic values between and within species",
    "section": "2.6 Results ",
    "text": "2.6 Results \n\n2.6.1 Dummy variables\nLabels for groups are assigned alphabetically, starting at 0 (Python’s indices start at 0): - Species: - H. dipterurus: 0 - N. entemedor: 1 - R. steindachneri: 2 - Season: - Cold: 0 - Warm: 1 - Sex: - Female: 0 - Male: 1 - Maturity: - Adult: 0 - Juvenile: 1\n\nref_data['Species_codes'] = pd.Categorical(ref_data['Species']).codes\nref_data['Season_codes'] = pd.Categorical(ref_data['Season']).codes\nref_data['Sex_codes'] = pd.Categorical(ref_data['Sex']).codes\nref_data['Maturity_codes'] = pd.Categorical(ref_data['Maturity']).codes\n\nref_data.drop(columns = ['d15n', 'd13c'])\n\n\n\n\n\n\n\n\nSpecies\nYear\nSeason\nSex\nMaturity\nSpecies_codes\nSeason_codes\nSex_codes\nMaturity_codes\n\n\n\n\n0\nH.dipterurus\n2015\nWarm\nMale\nJuvenile\n0\n1\n1\n1\n\n\n1\nH.dipterurus\n2015\nWarm\nFemale\nJuvenile\n0\n1\n0\n1\n\n\n2\nH.dipterurus\n2015\nWarm\nMale\nAdult\n0\n1\n1\n0\n\n\n3\nH.dipterurus\n2015\nWarm\nFemale\nJuvenile\n0\n1\n0\n1\n\n\n4\nH.dipterurus\n2015\nWarm\nMale\nAdult\n0\n1\n1\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n219\nR.steindachneri\n2016\nWarm\nMale\nJuvenile\n2\n1\n1\n1\n\n\n220\nR.steindachneri\n2016\nWarm\nMale\nAdult\n2\n1\n1\n0\n\n\n221\nR.steindachneri\n2016\nWarm\nMale\nAdult\n2\n1\n1\n0\n\n\n222\nR.steindachneri\n2016\nWarm\nMale\nAdult\n2\n1\n1\n0\n\n\n223\nR.steindachneri\n2016\nWarm\nFemale\nAdult\n2\n1\n0\n0\n\n\n\n\n224 rows × 9 columns\n\n\n\n\n\n2.6.2 Isotopic means\nThe blue trace corresponds to H. dipterurus, the yellow to N. entemedor and the green to R. steindachneri.\n\n\\(\\mu\\): Posterior distribution of the means of each species for each isotope. They are among the higher levels on the hierarchy, hence including the uncertainty in the other estimates:\n\n\\(\\mu_{\\delta^{13}C}\\): Shows the separation between species in \\(\\delta^{13}C\\). N. entemedor shows intermediate values among the other two species.\n\\(\\mu_{\\delta^{15}N}\\): Ídem.\n\n\n\naz.plot_trace(full_mod, var_names = 'µδ', filter_vars = 'like');\n\n\n\n\nFigure 2.9: Trace of the isotopic means for each species.\n\n\n\n\n\n\n2.6.3 Effect of the sex\n\n\\(Sex\\_\\): Slope for the sexes of each species for each isotope.\n\nCodes: Female = 0, Male = 1\nThe difference between sexes is small (\\(\\approx 0\\)‰).\n\n\n\naz.plot_trace(full_mod, var_names = 'Sex', filter_vars = 'like');\n\n\n\n\nFigure 2.10: Trace for the slope for the sex.\n\n\n\n\n\n\n2.6.4 Effect of the season\n\n\\(Season\\_\\): Slope for the season:\n\nCodes: Cold = 0, Warm = 1\nH. dipterurus: Difference between seasons of \\(\\approx 1\\)‰ in \\(\\delta^{13}C\\) and \\(\\approx 2\\)‰ in \\(\\delta^{15}N\\).\nN. entemedor: \\(\\Delta^{15}N \\in (-1,0)\\)‰\nThe posterior distributions of R. steindachneri are very broad, possibly due to the limited sample size of three for the cold season.\n\n\n\naz.plot_trace(full_mod, var_names = 'Season', filter_vars = 'like');\n\n\n\n\nFigure 2.11: Trace for the slope for the seasons.\n\n\n\n\n\n\n2.6.5 Effect of the maturity stage\n\n\\(Maturity\\_\\): Slope value for the age category of each isotope for each species:\n\nCodes: Adult = 0, Juvenile = 1\nH. dipterurus \\(\\Delta^{13}C \\in (0,2)\\), \\(\\Delta^{15}N \\in (-1.5,0]\\)\nN. entemedor \\(\\Delta^{15}N \\in [-1.5,0)\\)\nR. steindachneri: \\(\\Delta^{13}C \\in (-1,0)\\)\n\n\n\naz.plot_trace(full_mod, var_names = 'Maturity', filter_vars = 'like');\n\n\n\n\nFigure 2.12: Trace for the slope for the maturity stages.\n\n\n\n\n\n\n2.6.6 Inference \nThe results are summarized and further interpreted below, evaluating the \\(P(\\beta \\neq 0)\\), based on that if \\(P(\\beta < 0)\\) or \\(P(\\beta > 0) \\approx 50\\%\\) the \\(P(\\beta \\neq 0)\\) is very low (\\(\\sim 50\\%\\) of the distribution on either side of 0).\n\n2.6.6.1 Sexes \nIn general, the effect is small, with probabilities smaller than 75% in most cases. The only exception was in \\(\\delta^{13}C\\) for H. dipterurus with \\(P(\\beta < 0) \\approx 90\\%\\), suggesting that females possibly had more coastal habits than males (\\(\\bar{\\beta} = -0.32\\)‰).\n\nInterpretation:\n\n\\(\\delta^{13}C_f = \\delta^{13}C + \\beta*0 = \\delta^{13}C\\)\n\\(\\delta^{13}C_m = \\delta^{13}C + \\beta*1 = \\delta^{13}C - 0.32\\)‰; i.e., males had more negative values.\n\n\n\naz.plot_posterior(full_mod, var_names = 'Sex', filter_vars = 'like', ref_val = 0, hdi_prob = 0.95);\n\n\n\n\nFigure 2.13: Posterior distributions of the effect of the sex.\n\n\n\n\n\n\n2.6.6.2 Season \nThe effect of the season was more evident, with \\(P(\\beta < 0 \\lor \\beta > 90)\\%\\) in most comparisons:\n\n\\(\\delta^{13}C\\):\n\nH. dipterurus: Cold season with less negative values than the warm season, suggesting more oceanic habits during the latter.\n\n\\(\\delta^{13}C_w = \\delta^{13}C - \\beta\\)\n\\(\\delta^{13}C_c = \\delta^{13}C\\); cold season with less negative values.\n\n\n\\(\\delta^{15}N\\): The three species showed less positive values during the cold season in relation to the warm season, which suggests that their prey had a higher trohpic position during the latter.\n\n\\(\\delta^{13}C_w = \\delta^{13}C + \\beta\\)\n\\(\\delta^{13}C_c = \\delta^{13}C\\); cold season with less positive values.\n\n\nThe lowest \\(P(\\beta < 0 \\lor \\beta > 90)\\%\\) were found for N. entemedor (\\(P \\approx 70\\%\\)) and R. steindachneri (\\(P \\approx 60%\\)) in \\(\\delta^{13}C\\).\n\naz.plot_posterior(full_mod, var_names = 'Season', filter_vars = 'like', ref_val = 0, hdi_prob = 0.95);\n\n\n\n\nFigure 2.14: Posterior distributions of the effect of the season.\n\n\n\n\n\n\n2.6.6.3 Maturity stages \nThere were high probabilities of differences between age categories, with the exception of R. steindachneri en \\(\\delta^{15}N\\) (\\(P(\\beta>0) \\approx 56\\%\\)). The rest of the comparisons had similar trends, in the sense that juveniles had lower values in both isotopes, suggesting more oceanic habitats and lower trophic positions. The only exception was H. dipterurus in \\(\\delta^{13}C\\), whose juveniles had more coastal habits (less negative values).\n\nInterpretation a):\n\n\\(\\delta^{13}C\\):\n\n\\(\\delta^{13}C_a = \\delta^{13}C\\)\n\\(\\delta^{13}C_j = \\delta^{13}C - \\beta\\); juveniles had more negative values.\n\n\\(\\delta^{15}N\\):\n\n\\(\\delta^{13}C_a = \\delta^{15}N\\)\n\\(\\delta^{13}C_j = \\delta^{15}N - \\beta\\); juveniles had less positive values.\n\n\nInterpretation b):\n\n\\(\\delta^{13}C_a = \\delta^{13}C\\)\n\\(\\delta^{13}C_j = \\delta^{13}C + \\beta\\); juveniles had more positive values.\n\n\n\naz.plot_posterior(full_mod, var_names = 'Maturity', filter_vars = 'like',\n                  ref_val = 0, hdi_prob = 0.95);\n\n\n\n\nFigure 2.15: Posterior distributions of the effect of the maturity stage.\n\n\n\n\n\n\n\n2.6.7 Together\nThese results can be presented in a more compact way using forest plots, where the lines represent the Highest Density Intervals (also called Credible Intervals). Thin lines represent the \\(HDI_{95\\%}\\), thick lines the \\(HDI_{50\\%}\\), and the points the mean of the distribution.\n\naz.plot_forest(full_mod, hdi_prob = 0.95, var_names = '_δ15N', filter_vars = 'like', combined = True);\n#plt.gcf().set_size_inches(4*1.61,4.5)\n#plt.savefig(output_dir+'/figures/d15N_forestplot.pdf', format = 'pdf', bbox_inches = 'tight')\n\n\n\n\nFigure 2.16: Forest plot of the effects of the covariates in the d15N values.\n\n\n\n\n\naz.plot_forest(full_mod, hdi_prob = 0.95, var_names = '_δ13C', filter_vars = 'like', combined = True);\n#plt.gcf().set_size_inches(4*1.61,4.5)\n#plt.savefig(output_dir+'/figures/d13C_forestplot.pdf', format = 'pdf', bbox_inches = 'tight')\n\n\n\n\nFigure 2.17: Forest plot of the effects of the covariates in the d13C values.\n\n\n\n\n\n2.6.7.1 Isotopic mean comparison between species \nSince in the process we also estimated the posterior distributions of the means for each species, we can also directly compare them by simply substracting them:\n\ndef compare_posteriors(mod, labels,\n                       pars = ['µδ13C', 'µδ15N'],\n                       isos = ['$\\delta^{13}C$', '$\\delta^{15}N$'],\n                       save = False):\n    from itertools import combinations\n    # Get every comparison\n    spp = [i for i, sp in enumerate(labels)]\n    comps = list(combinations(spp, 2))\n    # Create a figure with two spaces for each combination\n    for i, comp in enumerate(comps):\n        _, ax = plt.subplots(1, 2, figsize = (10,6), constrained_layout = False,\n                             sharex = False, sharey = False)\n        # For each parameter (isotope):\n        for j,par in enumerate(pars):\n            # Calculate the difference between posterior distributions\n            Diff = mod.posterior[par][:,:,comp[0]] - mod.posterior[par][:,:,comp[1]]\n            # Plot the difference, show the median of the distribution and P(Diff < 0 or Diff > 0)\n            az.plot_posterior(Diff, ref_val = 0, hdi_prob = 0.95, point_estimate = 'mean', ax = ax[j])\n            # Title for each panel is the isotope\n            ax[j].set_title(f'{isos[j]}')\n            # The title for each pair of comparisons is the pair of species\n            plt.suptitle(f'{labels[comp[0]]} vs. {labels[comp[1]]}')\n            if save is True:\n                plt.gcf().set_size_inches(4*1.61, 4)\n                plt.savefig(output_dir+'/figures/'+f'{labels[comp[0]]} vs. {labels[comp[1]]}.pdf',\n                            format = 'pdf',\n                            bbox_inches = 'tight')\n\n\ndef forest_posteriors(mod, labels,\n                      pars = ['µδ13C', 'µδ15N'],\n                      isos = ['$\\delta^{13}C$', '$\\delta^{15}N$'],\n                      var_names = None,\n                      save = False):\n    from itertools import combinations\n    from xarray import Dataset, concat, merge\n    # Get every comparison\n    spp = [i for i, sp in enumerate(labels)]\n    comps = list(combinations(spp, 2))\n    Diffs = list()\n    comps_sp = list()\n    for j, par in enumerate(pars):\n        for i,comp in enumerate(comps):\n            # Calculate the difference between posterior distributions\n            Diff = mod.posterior[par][:,:,comp[0]] - mod.posterior[par][:,:,comp[1]]\n            Diffs.append(Diff)\n\n    comps_sp = [f'{labels[comp[0]]} - {labels[comp[1]]}' for comp in comps]\n    x_array = concat(Diffs, dim = 'iso')\n    x_array = x_array.values.reshape((2,3,3,5000))\n    x_array =  Dataset(data_vars = {pars[0]: (['comp', 'chain', 'draw'], x_array[0,:,:,:]),\n                                    pars[1]: (['comp', 'chain', 'draw'], x_array[1,:,:,:])},\n                       coords = {'iso': ['µδ13C', 'µδ15N'], 'comp': comps_sp})\n    #comps_sp = set(comps_sp)\n    \n    az.plot_forest(x_array, combined = True,\n                   #kind = 'ridgeplot',\n                   #ridgeplot_alpha = 0,\n                   #ridgeplot_truncate = False,\n                   #ridgeplot_quantiles = [0.05, 0.5, 0.95],\n                   hdi_prob = 0.95, var_names = var_names)\n\nIn the following figures we can observe high probabilities of mean differences in almost every comparison. Summarising:\n\n\\(\\delta^{13}C\\)\n\nN. entemedor > H. dipterurus (\\(P = 100\\%\\))\nH. dipterurus < R. steindachneri (\\(P \\approx 54.4\\%\\))\nN. entemedor > R. steindachneri (\\(P = 100\\%\\))\n\n\\(\\delta^{15}N\\)\n\nN. entemedor < H. dipterurus (\\(P = 100\\%\\))\nH. dipterurus > R. steindachneri (\\(P \\approx 100\\%\\))\nN. entemedor > R. steindachneri (\\(P = 100\\%\\))\n\n\n\n\ncompare_posteriors(mod = full_mod,\n                   labels = ['H. dipterurus', 'N. entemedor', 'R. steindachneri'],\n                   save = False)\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n(c)\n\n\n\nFigure 2.18: Posterior distributions of the differences between mean isotopic values among species.\n\n\n\nforest_posteriors(full_mod,\n                  labels = ['HD', 'NE', 'RS'])\n#plt.gcf().set_size_inches(4*1.61, 4)\n#plt.savefig(output_dir+'/figures/mean_forestplot.pdf',\n#            transparent = True,\n#            format = 'pdf', bbox_inches = 'tight')\n\n\n\n\nFigure 2.19: Forest plot of the posterior distributions of the differences between mean isotopic values among species.\n\n\n\n\n\n\n\n2.6.8 Bivariate Posterior Distribution \n\ndef plot_posteriors(data, mod, labels = None, group_col = 'Species', iso_cols = ['d13c', 'd15n'],\n                    palette = 'Paired', shade = True,\n                    pars = ['µδ13C', 'µδ15N'], isos = ['$\\delta^{13}C$', '$\\delta^{15}N$']):\n    \n    from matplotlib.ticker import MaxNLocator\n    # To avoid re-indexing every time\n    x = iso_cols[0]\n    y = iso_cols[1]\n    \n    # Empty Data Frame to store the posterior samples\n    samples = pd.DataFrame()\n    \n    # Unique values of the group column to cicle through\n    groups = data[group_col].unique()\n    \n    # Define limits of the plot\n    xlim = (data[x].min() - 0.5, data[x].max() + 0.5)\n    ylim = (data[y].min() - 0.5, data[y].max() + 0.5)\n    \n    # Form a Data Frame with the posterior samples for each isotope\n    shape = mod.posterior[pars[0]].shape\n    for group in groups:\n            temp = pd.DataFrame({'d13c':mod.posterior[pars[0]][:,:,group].values.reshape(shape[0]*shape[1]),\n                                 'd15n':mod.posterior[pars[1]][:,:,group].values.reshape(shape[0]*shape[1]),\n                                 group_col: group})\n            samples = samples.append(temp, ignore_index = False)\n            \n    samples = samples.reset_index(drop = True)\n    \n    # Map value labels to a new column (if provided)\n    if type(labels) is dict:\n        data['Group'] = data[group_col].map(labels)\n        samples['Group'] = samples[group_col].map(labels)\n        group_col = 'Group'\n    \n    # Plot the joint posterior distribution and its marginals\n    grid = sns.JointGrid(x = x, y = y, data = data, xlim = xlim, ylim = ylim)\n    g = grid.plot_joint(sns.scatterplot, hue = group_col, palette = palette, data = data, alpha = 0.7)\n    sns.kdeplot(x = samples[iso_cols[0]], shade = True, thresh = 0.05, hue = samples[group_col], \n                ax = g.ax_marg_x, palette = palette, legend = False)\n    sns.kdeplot(y = samples[iso_cols[1]], shade = True, thresh = 0.05, hue = samples[group_col], \n                ax = g.ax_marg_y, palette = palette, legend = False)\n    sns.kdeplot(x = samples[iso_cols[0]], y = samples[iso_cols[1]], shade = shade, thresh = 0.05, \n                hue = samples[group_col], palette = palette, legend = True, cbar= False, \n                alpha = 0.7, ax = g.ax_joint)\n    \n    # Change the appearance of the plot\n    g.ax_joint.set_xlabel(isos[0])\n    g.ax_joint.set_ylabel(isos[1])\n    g.ax_joint.xaxis.set_major_locator(MaxNLocator(5))\n    g.ax_joint.yaxis.set_major_locator(MaxNLocator(5))   \n\nGiven that both isotopes were modeled at the same time, we can plot the joint posterior mean distribution, which is non-orhtogonal as if both isotopes were independent from one another (separate Generalized Linear Models or independent null hypothesis testing). It is important to remark that the means are not in the centroid of each group. This is a favorable consequence of having used Student-T distributions for the parameters and likelihood instead of normal distributions. By stablishing that extreme values have a higher probability than in a normal distribution, the effect of the more extreme values is disregarded or, in another words, in a normal distribution they have low probabilities and, hence, each point is given a heavier weight.\nThis consequence is why using heavy-tailed distributions is called robust regression. In this example the effect is shown in a linear regression with a continuous covariate, in which the robust estimation of the parameters is not “deceived” by the more extreme values on the y variable and posterior predictive lines are closer to the real line than those of the “normal” regression.\n\nplot_posteriors(data = glm_data, mod = full_mod,\n                labels = {0: 'H. dipterurus', 1: 'N. entemedor', 2: 'R. steindachneri'},\n                palette = colors[0:3])\n#plt.gcf().set_size_inches(4, 4*1.61)\n#plt.savefig(output_dir+'/figures/posterior_means.pdf', format = 'pdf', bbox_inches = 'tight')\n\n\n\n\nFigure 2.20: Bivariate posterior distributions of the mean isotopic values.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nEnd of notebook\n\n\n\n%load_ext watermark\n%watermark -n -u -v -iv -w\n\nLast updated: Sat Jun 03 2023\n\nPython implementation: CPython\nPython version       : 3.8.6\nIPython version      : 7.19.0\n\nseaborn   : 0.11.0\nmatplotlib: 3.3.3\nnumpy     : 1.19.4\npymc3     : 3.11.2\narviz     : 0.11.2\ntheano    : 1.1.2\npandas    : 1.1.5\n\nWatermark: 2.1.0\n\n\n\n\n\n\n\nArmhein V, Greenland S, McShane B (2019) Scientists Rise up Against Statistical Significance. Nature 567:305–307.\n\n\nBetancourt M (2017) A conceptual introduction to Hamiltonian Monte Carlo. arXiv preprint 1701.02434.\n\n\nBolstad WM (2004) Introduction to bayesian statistics. Wiley-Interscience, New Jersey, USA.\n\n\nEllison AM (2004) Bayesian Inference in Ecology. Ecology Letters 7:509–520.\n\n\nGabry J, Simpson D, Vehtari A, Betancourt M, Gelman A (2017) Visualization in Bayesian workflow. arXiv preprint 1709.01449.\n\n\nGerrodette T (2011) Inference Without Significance: Measuring Support for Hypotheses Rather Than Rejecting Them. Marine Ecology 32:404–418.\n\n\nHobbs NT, Hilborn R (2006) Alternatives to Statistical Hypothesis Testing in Ecology: A Guide to Self Teaching. Ecological Applications 16:5–19.\n\n\nJackson AL, Inger R, Parnell AC, Bearhop S (2011) Comparing Isotopic Niche Widths Among and Within Communities: SIBER-Stable Isotope Bayesian Ellipses in R. Journal of Animal Ecology 34:595–602.\n\n\nKruschke JK (2012) Bayesian Estimation Supersedes the t Test. Journal of Experimental Psychology: General 142:573–603.\n\n\nKruschke JK (2015) Doing bayesian data analysis: A tutorial with r, JAGS, and stan, 2nd ed. Academic Press, London, UK.\n\n\nLewandowski D, Kurowicka D, Joe H (2009) Generating Random Correlation Matrices Based on Vines and Extended Onion Method. Journal of Multivariate Analysis 100:1989–2001.\n\n\nLink WA, Eaton MJ (2012) On Thinning of Chains in MCMC. Methods in Ecology and Evolution 3:112–115.\n\n\nMartin O (2018) Bayesian analysis with python: Introduction to statistical modeling and probabilistic programming using PyMC3 and ArviZ, 2nd ed. Pakt Publishing.\n\n\nPark T, Casella G (2012) The Bayesian Lasso. Journal of the American Statistical Association 103:681–686.\n\n\nSalvatier J, Wiecki TV, Fonnesbeck C (2016) Probabilistic programming in Python using PyMC3. PeerJ Computer Science 2:e55.\n\n\nTeam SD (2022) Stan modeling language user’s guide and reference manual. https://mc-stan.org/docs/reference-manual/effective-sample-size.html\n\n\nVan Rossum G, Drake FL (2009) Python 3 reference manual. CreateSpace, Scotts Valley, CA."
  },
  {
    "objectID": "SI3_SIBER.html#libraries",
    "href": "SI3_SIBER.html#libraries",
    "title": "3  Amplitudes and comparisons of the isotopic niches",
    "section": "3.1 Libraries",
    "text": "3.1 Libraries\n\nlibrary(SIBER)\nlibrary(ggplot2)\nlibrary(ggExtra)\nlibrary(ggConvexHull)\nlibrary(HDInterval)\nlibrary(mcmcplots)\nlibrary(ggmcmc)\nlibrary(coda)\nlibrary(lattice)\nlibrary(MCMCvis)\nlibrary(bayesplot)"
  },
  {
    "objectID": "SI3_SIBER.html#data-loading",
    "href": "SI3_SIBER.html#data-loading",
    "title": "3  Amplitudes and comparisons of the isotopic niches",
    "section": "3.2 Data loading",
    "text": "3.2 Data loading\n\ndata <- read.table(\"data/glm.csv\", sep = \",\", header = T)\ncolnames(data)[6:7] <- c(\"iso1\", \"iso2\")"
  },
  {
    "objectID": "SI3_SIBER.html#global-siber",
    "href": "SI3_SIBER.html#global-siber",
    "title": "3  Amplitudes and comparisons of the isotopic niches",
    "section": "3.3 “Global” SIBER:",
    "text": "3.3 “Global” SIBER:\n\n3.3.1 Dataset arrangement\n\nglobal <- subset(data, select = c(sp, iso1, iso2))\nglobal$group <- factor(global$sp, labels = c(1:3))\nglobal$community <- 1\nglobal\n\n\n\n  \n\n\n\n\n\n3.3.2 Basic plot\nParameters and basic information\n\nglobal_colors <- c('#1f77b4', '#ff7f0e', '#2ca02c')\nxlims <- c(min(global$iso2)-0.5, max(global$iso2)+0.5)\nylims <- c(min(global$iso1)-0.5, max(global$iso1)+0.5)\nspecies <- unique(global$sp)\n\nPlot\n\nglob_ellipses <- ggplot(data = global, aes(x = iso2, y = iso1, color = as.factor(group))) +\n                 geom_point(alpha = 0.7) +\n                 stat_ellipse(level = 0.4) +\n                 geom_convexhull(fill = NA, linetype = \"dashed\", alpha = 0.05) +\n                 scale_color_manual(values = global_colors, name = \"Species\", labels = species) +\n                 scale_x_continuous(limits = xlims, breaks = c(seq(-18, -8, 2))) +\n                 scale_y_continuous(limits = ylims, breaks = c(seq(10, 20, 2))) +\n                 labs(title = element_blank(),\n                      x = expression({delta}^13*C~'(\\u2030)'),\n                      y = expression({delta}^15*N~'(\\u2030)')) +\n                 theme_bw() +\n                 theme(aspect.ratio = 1,\n                       panel.grid.major = element_blank(),\n                       panel.grid.minor = element_blank(),\n                       legend.position = c(0.20, 0.17),\n                       legend.background = element_blank())\n\n# cairo_pdf(\"output/SIBER/global/global_ellipses.pdf\",\n#           family = \"Times\", height = 3.5, width = 3.5)\nglob_ellipses\n\n\n\n# dev.off()\n\n\n\n3.3.3 Fitting the Bayesian Bi-variate Normal models:\nCreation of the SIBER object:\n\nsiber_glob <- createSiberObject(global[,2:5])\n\nParameters of the MCMC:\n\nparms <- list()\nparms$n.iter   <- 20000    \nparms$n.burnin <- 25000   \nparms$n.thin   <- 1          \nparms$n.chains <- 3\nparms$save.output <- TRUE\nparms$save.dir <- paste(getwd(),\"/output/SIBER/global\", sep = \"\")\n\nPrior distributions:\n\npriors <- list()\npriors$R <- 1 * diag(2)\npriors$k <- 2\npriors$tau.mu <- 1.0E-3\n\nSampling of the posterior distributions:\n\nellipses.posterior <- siberMVN(siber_glob, parms, priors)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 81\n   Unobserved stochastic nodes: 3\n   Total graph size: 96\n\nInitializing model\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 69\n   Unobserved stochastic nodes: 3\n   Total graph size: 84\n\nInitializing model\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 74\n   Unobserved stochastic nodes: 3\n   Total graph size: 89\n\nInitializing model\n\n\n\n3.3.3.1 Diagnostics:\nCycling through every model to extract information:\n\nall_files <- dir(parms$save.dir, full.names = T)\nmodel_files <- all_files[grep(\"jags_output\", all_files)]\n\nmodel_summary <- list()\n\nfor (i in seq_along(model_files)) {\n  load(model_files[i])\n  \n  mcmc_list <- coda::as.mcmc(do.call(\"cbind\", output))\n  \n  model_summary[[i]] <- MCMCsummary(output,\n                                    params = \"all\",\n                                    probs = c(0.025, 0.975),\n                                    round = 3)\n  MCMCtrace(output,\n            params = \"all\",\n            iter = parms$n.iter,\n            Rhat = T,\n            n.eff = T,\n            ind = T,\n            type = \"density\",\n            open_pdf = F,\n            filename = sprintf(\"output/SIBER/global/MCMCtrace_%d\", i))\n}\n\nAutocorrelation plots:\n\n# cairo_pdf(\"output/SIBER/global_autocorr.pdf\",\n#           family = \"Times\", height = 10, width = 12)\ngridExtra::grid.arrange(mcmc_acf(ellipses.posterior[1]) + labs(title = species[1]),\n                        mcmc_acf(ellipses.posterior[2]) + labs(title = species[2]),\n                        mcmc_acf(ellipses.posterior[3]) + labs(title = species[3]),\n                        nrow = 3)\n\n\n\n#dev.off()\n\nSummaries. Gelman-Rubick Statistics (Rhat) exactly equal to 1 and effective sample sizes > 2000 for every parameter:\n\nfor (i in seq_along(model_files)) {\n  load(model_files[i])\n  print(species[i])\n  print(model_summary[[i]])\n}\n\n[1] \"H.dipterurus\"\n              mean    sd   2.5%  97.5% Rhat n.eff\nSigma2[1,1]  1.025 0.165  0.751  1.393    1 60000\nSigma2[2,1] -0.557 0.132 -0.851 -0.331    1 59779\nSigma2[1,2] -0.557 0.132 -0.851 -0.331    1 59779\nSigma2[2,2]  1.025 0.165  0.753  1.392    1 58288\nmu[1]        0.000 0.112 -0.221  0.220    1 31856\nmu[2]        0.000 0.112 -0.220  0.221    1 31680\n[1] \"N.entemedor\"\n              mean    sd   2.5%  97.5% Rhat n.eff\nSigma2[1,1]  1.030 0.180  0.734  1.434    1 58420\nSigma2[2,1] -0.684 0.152 -1.025 -0.432    1 58657\nSigma2[1,2] -0.684 0.152 -1.025 -0.432    1 58657\nSigma2[2,2]  1.030 0.180  0.736  1.440    1 58409\nmu[1]        0.001 0.121 -0.240  0.239    1 23172\nmu[2]       -0.001 0.122 -0.240  0.238    1 23062\n[1] \"R.steindachneri\"\n              mean    sd   2.5%  97.5% Rhat n.eff\nSigma2[1,1]  1.028 0.174  0.743  1.420    1 59100\nSigma2[2,1] -0.773 0.154 -1.119 -0.519    1 58438\nSigma2[1,2] -0.773 0.154 -1.119 -0.519    1 58438\nSigma2[2,2]  1.029 0.174  0.742  1.426    1 57750\nmu[1]       -0.002 0.118 -0.234  0.229    1 16294\nmu[2]        0.002 0.118 -0.228  0.234    1 16515\n\n\n\n\n3.3.3.2 Inferences\nGeneration of the ellipses:\n\nsea_b <- siberEllipses(ellipses.posterior)\nsummary(sea_b)\n\n       V1               V2              V3       \n Min.   : 6.193   Min.   :1.323   Min.   :1.077  \n 1st Qu.: 8.875   1st Qu.:1.971   1st Qu.:1.541  \n Median : 9.555   Median :2.136   Median :1.666  \n Mean   : 9.636   Mean   :2.156   Mean   :1.681  \n 3rd Qu.:10.311   3rd Qu.:2.318   3rd Qu.:1.804  \n Max.   :15.689   Max.   :3.721   Max.   :3.167  \n\n\nGraphical comparisons. First, transform the object to a data.frame and melt it to long format:\n\nsea_bt <- reshape2::melt(as.data.frame(sea_b))\n\nNo id variables; using all as measure variables\n\ncolnames(sea_bt) <- c(\"sp\", \"SEAb\")\nhead(sea_bt)\n\n\n\n  \n\n\n\nGet the HDI for each species:\n\nhdi <- as.data.frame(t(apply(sea_b, 2, FUN = HDInterval::hdi)))\nhdi <- cbind(hdi, means = t(as.data.frame(t(apply(sea_b, 2, FUN = mean)))))\nhdi[\"sp\"] <- row.names(hdi)\nhdi\n\n\n\n  \n\n\n\nViolin plot with the posterior distributions of the SEAb:\n\nseab_global <- ggplot() +\n               geom_violin(data = sea_bt, aes(x = sp, y = SEAb, fill = sp, color = sp),\n                           show.legend = F, alpha = 0.3) +\n               geom_boxplot(data = sea_bt,\n                            aes(x = sp, y = SEAb, fill = sp, color = sp),\n                            alpha = 0.5, width = 0.05, notch = T, show.legend = F,\n                            outlier.shape = NA, coef = 0) +\n               scale_fill_manual(values = global_colors) +\n               scale_color_manual(values = global_colors) +\n               geom_linerange(data = hdi,\n                              aes(x = sp, ymin = lower, ymax = upper, color = sp),\n                              show.legend = F) +\n               theme_bw()+\n               theme(aspect.ratio = 1,\n                     panel.grid.major = element_blank(),\n                     panel.grid.minor = element_blank(),\n                     legend.position = c(0.2, 0.15),\n                     legend.background = element_blank()) +\n               labs(x = \"Species\",\n                    y = expression(\"SEAb \" ('\\u2030' ^2) )) +\n               scale_x_discrete(labels = species)\n# cairo_pdf(\"output/SIBER/global/global_seab.pdf\",\n#           family = \"Times\", height = 3.5, width = 3.5)\nseab_global\n\n\n\n#dev.off()\n\nPosterior Differences:\n\n# Define the comparisons\ncomps <- expand.grid(a = unique(as.integer(global$group)),\n                     b = unique(as.integer(global$group)))\ncomps <- comps[(comps$a != comps$b & comps$b > comps$a), ]\ncomps <- comps[order(comps$a),]\n\n# Put the results in different objects:\ndiffs <- data.frame()\ndiff_glob <- data.frame()\np_sup <- data.frame()\nfor (i in seq_along(comps$a)) {\n  a <- comps$a[i]\n  b <- comps$b[i]\n  comp <- paste(species[a], \"-\", species[b])\n  diff <- sea_b[, a] - sea_b[, b]\n  sup <- subset(diff, diff>0, select = diff)\n  \n  diffs <- rbind(diffs,\n                 data.frame(comp = comp,\n                            diff = diff))\n  diff_glob <- rbind(diff_glob,\n                     data.frame(comp = comp,\n                                mean = mean(diff),\n                                IQr = t(HDInterval::hdi(diff, credMass = 0.5)),\n                                hdi = t(HDInterval::hdi(diff, credMass = 0.95))\n                                )\n                     )\n  p_sup <- rbind(p_sup,\n                 data.frame(comp = comp,\n                            p_sup = (length(sup)/length(diff))*100))\n}\n\ndiff_glob$comp <- factor(diff_glob$comp, ordered = T)\n\nHDIs of differences\n\ndiff_glob[\"keys\"] <- factor(c(\"HD-NE\", \"HD-RS\", \"NE-RS\"), ordered = T)\ndiff_glob\n\n\n\n  \n\n\n\nProbabilities of differences:\n\np_sup\n\n\n\n  \n\n\n\nForestplot of the \\(HDI_{95\\%}\\) of the differences\n\nglobal_forest <- ggplot(data = diff_glob) +\n                 geom_vline(xintercept = 0, linetype = \"dashed\", color = \"#C9C8C8\", size = 1.2) +\n                 geom_linerange(aes(y = factor(comp, ordered = T),\n                                    xmin = IQr.lower, xmax = IQr.upper),\n                                color = global_colors[1], size = 1) +\n                 geom_linerange(aes(y = factor(comp, ordered = T),\n                                    xmin = hdi.lower, xmax = hdi.upper),\n                                color = global_colors[1]) +\n                 geom_point(aes(x = mean, y = factor(comp, ordered = T)),\n                            color = global_colors[1],\n                            fill = \"white\", size = 1.5, shape = 21) +\n                 theme_bw() +\n                 theme(aspect.ratio = 1,\n                       panel.grid.major = element_blank(),\n                       panel.grid.minor = element_blank(),\n                       legend.position = c(0.2, 0.15),\n                       legend.background = element_blank()) +\n                 labs(title = expression(\"Differences in SEAB \" ('\\u2030' ^2)),\n                      x = element_blank(),\n                      y = element_blank())\n\n# cairo_pdf(\"output/SIBER/global/global_forest.pdf\",\n#           family = \"Times\", height = 4, width = 4)\nglobal_forest\n\n\n\n# dev.off()\n\n\n\n3.3.3.3 Eccentricity and theta\nEccentricities represent the elongation of the ellipse, that is, how far they are from a perfect circle (0 eccentricity). Lower values indicate a rounder ellipse, while higher values indicate a more elongated ellipse.\n\nmat_ecc <- function(x) {\n  mat <- matrix(x, 2, 2)\n  ecc <- sigmaSEA(mat)$ecc\n  return(ecc)\n}\n\nmat_theta <- function(x){\n  mat <- matrix(x, 2, 2)\n  theta <- sigmaSEA(mat)$theta\n  return(theta)\n}\n\necc <- data.frame()\nfor (sp in seq_along(ellipses.posterior)) {\n  ecc <- rbind(ecc,\n               data.frame(sp = species[sp],\n                          ecc = apply(ellipses.posterior[[sp]][,1:4], 1, mat_ecc),\n                          theta = apply(ellipses.posterior[[sp]][,1:4], 1, mat_theta)))\n}\n\n\nec <- ecc %>% group_by(sp) %>% summarise(IQR.lower = hdi(ecc, credMass = 0.5)[1],\n                                         IQR.upper = hdi(ecc, credMass = 0.5)[2],\n                                         hdi.lower = hdi(ecc)[1],\n                                         hdi.upper = hdi(ecc)[2],\n                                         mean = mean(ecc))\nec\n\n\n\n  \n\n\n\n\nggplot(data = ecc, aes(x = sp, y = ecc, group = sp, color = factor(sp))) +\n  geom_violin(fill = NA, show.legend = F) +\n  geom_point(aes(x = sp, y = mean), data = ec, show.legend = F) +\n  #geom_errorbar(aes(x = sp, ymin = IQ.lower, ymax = IQ.upper, group = sp), data = ag) +\n  geom_boxplot(width = 0.1, show.legend = F) +\n  theme_bw() +\n  labs(title = \"SEAB Eccentricity\",\n       x = element_blank(),\n       y = element_blank()) +\n  scale_color_manual(values = global_colors) +\n  theme(aspect.ratio = 1,\n                       panel.grid.major = element_blank(),\n                       panel.grid.minor = element_blank(),\n                       legend.position = c(0.15, 0.1),\n                       legend.background = element_blank())\n\n\n\n\nTheta, on the other hand, is the angle of the semi-major axis of the SEA and the x axis; i.e., the angle of the deformation. Values close to 0 show a higher dispersion in \\(\\delta^{13}C\\), while values close to 90 show a higher dispersion in \\(\\delta^{15}N\\):\n\nth <- ecc %>% group_by(sp) %>% summarise(IQR.lower = hdi(theta, credMass = 0.5)[1],\n                                         IQR.upper = hdi(theta, credMass = 0.5)[2],\n                                         hdi.lower = hdi(theta)[1],\n                                         hdi.upper = hdi(theta)[2],\n                                         mean = mean(theta))\nth\n\n\n\n  \n\n\n\n\nggplot(data = ecc, aes(x = sp, y = theta, group = sp, color = factor(sp))) +\n  geom_violin(fill = NA, show.legend = F) +\n  geom_point(aes(x = sp, y = mean), data = ec, show.legend = F) +\n  #geom_errorbar(aes(x = sp, ymin = IQ.lower, ymax = IQ.upper, group = sp), data = ag) +\n  geom_boxplot(width = 0.1, show.legend = F) +\n  theme_bw() +\n  labs(title = \"SEAB theta\",\n       x = element_blank(),\n       y = element_blank()) +\n  scale_color_manual(values = global_colors) +\n  theme(aspect.ratio = 1,\n                       panel.grid.major = element_blank(),\n                       panel.grid.minor = element_blank(),\n                       legend.position = c(0.15, 0.1),\n                       legend.background = element_blank())"
  },
  {
    "objectID": "SI3_SIBER.html#intra-specific-analyses",
    "href": "SI3_SIBER.html#intra-specific-analyses",
    "title": "3  Amplitudes and comparisons of the isotopic niches",
    "section": "3.4 Intra-specific analyses",
    "text": "3.4 Intra-specific analyses\nBasic dataset to extract every case:\n\nfull_set <- subset(data,\n                   select = c(sp, temporada, sexo, estadio, iso1, iso2))\n\ncolnames(full_set) <- c(\"Species\", \"Season\", \"Sex\", \"Maturity\", \"iso1\", \"iso2\")\n\nhead(full_set)\n\n\n\n  \n\n\n\n\n3.4.1 Custom wrapper around SIBER:\n\ncustom_siber <- function(data, var_col, lbls, sp_col, colors, gp_label,\n                         n.burnin = 30000, n.iter = 2000){\n  \n  subdata <- data[,c(sp_col, var_col, \"iso1\", \"iso2\")]\n  subdata$group <- factor(subdata[,var_col], labels = seq_along(lbls))\n  subdata$community <- 1\n  \n  species <- unique(data[, sp_col])\n  \n  # Global directory for the variable\n  wd <- paste(getwd(), \"/output/SIBER/\", var_col, sep = \"\")\n  \n  # If the directory does not exist, create it:\n  if (dir.exists(wd) == F) {dir.create(wd)}\n  \n  # Empty lists to store the results:\n  siber_objs <- list()\n  ellipses <- list()\n\n  summaries <- list()\n  autocorr_plots <- list()\n  \n  sea_b <- list()\n  sea_bt <- data.frame()\n  hdis <- data.frame()\n  ecc <- data.frame()\n  \n  # Posterior differences\n  # Define the comparisons\n  comps <- expand.grid(a = unique(as.integer(subdata$group)),\n                       b = unique(as.integer(subdata$group)))\n  \n  comps <- comps[(comps$a != comps$b & comps$b > comps$a), ]\n  comps <- comps[order(comps$a),]\n  \n  # Put the results in different objects:\n  diffs <- data.frame()\n  diff_glob <- data.frame()\n  p_sup <- data.frame()\n  \n  # List with MCMC parameters:\n  parms <- list()\n    parms <- list()\n    parms$n.iter   <- n.iter    \n    parms$n.burnin <- n.burnin   \n    parms$n.thin   <- 1          \n    parms$n.chains <- 3\n    parms$save.output <- TRUE\n    \n  # List with prior parameters:\n  priors <- list()\n    priors$R <- 1 * diag(2)\n    priors$k <- 2\n    priors$tau.mu <- 1.0E-3\n    \n    \n  # Basic ellipses plot\n  ellipses <- ggplot(data = subdata, aes(x = iso2, y = iso1, color = as.factor(group))) +\n                     geom_point(alpha = 0.5) +\n                     stat_ellipse(level = 0.4) +\n                     geom_convexhull(fill = NA, linetype = \"dashed\", alpha = 0.05) +\n                     scale_color_manual(values = global_colors, name = gp_label, labels = lbls) +\n                     scale_x_continuous(limits = xlims, breaks = c(-18, -14, -10)) +\n                     scale_y_continuous(limits = ylims, breaks = c(12, 16, 20)) +\n                     labs(title = element_blank(),\n                          x = expression({delta}^13*C~'(\\u2030)'),\n                          y = expression({delta}^15*N~'(\\u2030)')) +\n                     theme_bw() +\n                     theme(aspect.ratio = 1,\n                           panel.grid.major = element_blank(),\n                           panel.grid.minor = element_blank(),\n                           #legend.position = c(0.2, 0.15),\n                           legend.background = element_blank()) +\n                     facet_wrap(paste(\"~\", sp_col, sep = \"\"))\n  \n  cairo_pdf(paste(wd, \"ellipses.pdf\", sep = \"/\"), family = \"Times\", height = 2.7, width = 5.4)\n    print(ellipses)\n  dev.off()\n  \n  # Loop through species:\n    for (i in seq_along(species)) {\n      \n      # Output directory:\n      gpwd <- paste(wd, \"/\", species[i], sep = \"\")\n      if (dir.exists(gpwd) == F) {dir.create(gpwd)}\n      parms$save.dir <- gpwd\n      \n      # SIBER objects\n      siber_objs[[species[i]]] <- createSiberObject(subdata[subdata[, sp_col] == species[i],3:6])\n      \n      # Fitting the Bayesian models:\n      ellipses[[species[i]]] <- siberMVN(siber_objs[[i]], parms, priors)\n      \n      # Diagnostics\n      ## Get all the files in the directory\n      all_files <- dir(parms$save.dir, full.names = T)\n      model_files <- all_files[grep(\"jags_output\", all_files)]\n      \n      # Empty lists to store the results\n      summaries[[species[i]]] <- list()\n      autocorr_plots[[species[i]]] <- list()\n      \n      # For each model:\n      for (j in seq_along(model_files)) {\n        # Load the model output\n        load(model_files[j])\n        # Create a list with the output\n        mcmc_list <- coda::as.mcmc(do.call(\"cbind\", output))\n        \n        # Store the summary for each model\n        summaries[[species[i]]][[j]]<- MCMCsummary(output,\n                                                   params = \"all\",\n                                                   probs = c(0.025, 0.975),\n                                                   round = 3)\n        # Write the summary to a csv file\n        write.csv(summaries[[species[i]]][[j]],\n                  paste(gpwd, sprintf(\"summary_%s.csv\",\n                                      lbls[j]), sep = \"/\"),\n                  row.names = T)\n        \n        # Plot the trace in a pdf\n        MCMCtrace(output,\n                  params = \"all\",\n                  iter = parms$n.iter,\n                  Rhat = T,\n                  n.eff = T,\n                  ind = T,\n                  type = \"density\",\n                  open_pdf = F,\n                  #filename = paste(gpwd, sprintf(\"/MCMCtrace_%d\", j), sep = \"\"))\n                  filename = sprintf(\"MCMCtrace_%s\", lbls[j]),\n                  wd = gpwd)\n        \n        # Store the autocorrelation plots\n        autocorr_plots[[species[i]]][[j]] <- mcmc_acf(ellipses[[species[i]]][j]) + labs(title = lbls[j])\n        \n        \n      } # End of models loop\n      \n      # Autocorrelation plots for every model for every species\n      cairo_pdf(paste(gpwd, \"/autocorr.pdf\", sep = \"\"), family = \"Times\")\n        gridExtra::grid.arrange(grobs = autocorr_plots[[species[i]]], nrow = 2)\n      dev.off()\n      \n      # Generation of the ellipses\n      sea_b[[species[i]]] <- siberEllipses(ellipses[[species[i]]])\n      \n      # Fill the data.frame with the SEAb data\n      sea_bt <- rbind(sea_bt,\n                      cbind(reshape2::melt(as.data.frame(sea_b[[species[i]]])),\n                            sp_col = species[i]))\n      if (i == max(seq_along(species))) {colnames(sea_bt) <- c(var_col, \"SEAb\", sp_col)}\n      \n      \n      # Create a data.frame to store the hdis\n      # Calculate posterior means\n      means <- t(as.data.frame(t(apply(sea_b[[species[i]]], 2, FUN = mean))))\n      # Calculate hdis\n      hdi <- rbind(as.data.frame(t(apply(sea_b[[species[i]]], 2, FUN = HDInterval::hdi))))\n      # Put the results in a data.frame\n      temp <- data.frame(hdi, means, sp = species[i])\n      \n      # Fill the data.frame with the results\n      hdis <- rbind(hdis, temp)\n      \n      if (i == max(seq_along(species))) {hdis[var_col] <- paste(\"V\", rep(seq_along(model_files)), sep = \"\")}\n      \n      # Compute the posterior SEAb differences\n      for (k in seq_along(comps$a)) {\n        a <- comps$a[k]\n        b <- comps$b[k]\n        comp <- paste(lbls[a], \"-\", lbls[b])\n        diff <- sea_b[[species[i]]][, a] - sea_b[[species[i]]][, b]\n        sup <- subset(diff, diff>0, select = diff)\n  \n        diffs <- rbind(diffs,\n                       data.frame(comp = comp,\n                                  diff = diff,\n                                  sp = species[i]))\n        diff_glob <- rbind(diff_glob,\n                           data.frame(comp = comp,\n                                      mean = mean(diff),\n                                      IQr = t(HDInterval::hdi(diff, credMass = 0.5)),\n                                      hdi = t(HDInterval::hdi(diff, credMass = 0.95)),\n                                      sp = species[i]\n                                      )\n                           )\n        p_sup <- rbind(p_sup,\n                       data.frame(comp = comp,\n                                  p_sup = (length(sup)/length(diff))*100,\n                                  sp = species[i]))\n    }\n    \n    diff_glob$comp <- factor(diff_glob$comp, ordered = T)\n      \n    }# End of species loop\n    \n    # Violin plot of SEAb\n    seab <- ggplot() +\n              geom_violin(data = sea_bt, aes_string(x = var_col, y = \"SEAb\", fill = var_col, color = var_col),\n                          show.legend = F, alpha = 0.3) +\n              geom_boxplot(data = sea_bt,\n                           aes_string(x = var_col, y = \"SEAb\", fill = var_col, color = var_col),\n                           alpha = 0.5, width = 0.05, notch = T, show.legend = F,\n                           outlier.shape = NA, coef = 0) +\n              scale_fill_manual(values = global_colors) +\n              scale_color_manual(values = global_colors) +\n              # geom_linerange(data = hdi,\n              #                aes_string(x = var_col, ymin = \"lower\", ymax = \"upper\", color = var_col),\n              #                show.legend = F) + # Fix needed\n              theme_bw() +\n              theme(aspect.ratio = 1,\n                    panel.grid.major = element_blank(),\n                    panel.grid.minor = element_blank(),\n                    legend.position = c(0.2, 0.15),\n                    legend.background = element_blank()) +\n              labs(x = gp_label,\n                   y = expression(\"SEAb \" ('\\u2030' ^2) )) +\n              scale_x_discrete(labels = lbls) +\n              facet_wrap(paste(\".~\", sp_col, sep = \"\"), ncol = 3, scales = \"free_y\")\n\n      cairo_pdf(paste(wd, \"SEAb.pdf\", sep = \"/\"), family = \"Times\", height = 4, width = 8)\n        print(seab)\n      dev.off()\n      \n    print(\"All models have been run. Check your working directory for the results\")\n    return(list(summaries = summaries,\n                sea_b = sea_b,\n                sea_bt = sea_bt,\n                hdis = hdis,\n                diffs = diffs,\n                diff_glob = diff_glob,\n                p_sup = p_sup,\n                ellips = ellipses))\n}\n\n\n\n3.4.2 Sex\n\nlbls <- c(\"Female\", \"Male\")\nsex_siber <- custom_siber(full_set, var_col = \"Sex\", lbls, sp_col = \"Species\", n.burnin = 7000,\n                          colors = global_colors, gp_label = \"Sex\")\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 37\n   Unobserved stochastic nodes: 3\n   Total graph size: 52\n\nInitializing model\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 44\n   Unobserved stochastic nodes: 3\n   Total graph size: 59\n\nInitializing model\n\n\nNo id variables; using all as measure variables\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 55\n   Unobserved stochastic nodes: 3\n   Total graph size: 70\n\nInitializing model\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 14\n   Unobserved stochastic nodes: 3\n   Total graph size: 29\n\nInitializing model\n\n\nNo id variables; using all as measure variables\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 31\n   Unobserved stochastic nodes: 3\n   Total graph size: 46\n\nInitializing model\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 43\n   Unobserved stochastic nodes: 3\n   Total graph size: 58\n\nInitializing model\n\n\nNo id variables; using all as measure variables\n\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation ideoms with `aes()`\n\n\n[1] \"All models have been run. Check your working directory for the results\"\n\n\nPlots of ellipses:\n\nsex_siber$ellips\n\n\n\n\nEccentricities:\n\necc <- data.frame()\nfor (sp in seq_along(species)) {\n  for (j in seq_along(lbls)) {\n    ecc <- rbind(ecc,\n               data.frame(sp = species[sp],\n                          j = lbls[j],\n                          ecc = apply(sex_siber$ellips[[species[sp]]][[j]][,1:4], 1, mat_ecc),\n                          theta = apply(sex_siber$ellips[[species[sp]]][[j]][,1:4], 1, mat_theta)))\n    \n  }\n}\n\n\nec <- ecc %>% group_by(sp, j) %>% summarise(IQR.lower = hdi(ecc, credMass = 0.5)[1],\n                                            IQR.upper = hdi(ecc, credMass = 0.5)[2],\n                                            hdi.lower = hdi(ecc)[1],\n                                            hdi.upper = hdi(ecc)[2],\n                                            mean = mean(ecc))\n\n`summarise()` has grouped output by 'sp'. You can override using the `.groups`\nargument.\n\nec\n\n\n\n  \n\n\n\n\nggplot(data = ecc, aes(x = j, y = ecc, group = j, color = factor(j))) +\n  geom_violin(fill = NA, show.legend = F) +\n  #geom_point(aes(x = sp, y = mean), data = ag, show.legend = F) +\n  #geom_errorbar(aes(x = sp, ymin = IQ.lower, ymax = IQ.upper, group = sp), data = ag) +\n  geom_boxplot(width = 0.1, show.legend = F) +\n  theme_bw() +\n  labs(title = \"SEAB Eccentricity\",\n       x = element_blank(),\n       y = element_blank()) +\n  scale_color_manual(values = global_colors) +\n  theme(aspect.ratio = 1,\n                       panel.grid.major = element_blank(),\n                       panel.grid.minor = element_blank(),\n                       legend.position = c(0.15, 0.1),\n                       legend.background = element_blank()) +\n  facet_wrap(~sp)\n\n\n\n\nTheta:\n\nth <- ecc %>% group_by(sp, j) %>% summarise(IQR.lower = hdi(theta, credMass = 0.5)[1],\n                                            IQR.upper = hdi(theta, credMass = 0.5)[2],\n                                            hdi.lower = hdi(theta)[1],\n                                            hdi.upper = hdi(theta)[2],\n                                            mean = mean(theta))\n\n`summarise()` has grouped output by 'sp'. You can override using the `.groups`\nargument.\n\nth\n\n\n\n  \n\n\n\n\nggplot(data = ecc, aes(x = j, y = theta, group = j, color = factor(j))) +\n  geom_violin(fill = NA, show.legend = F) +\n  #geom_point(aes(x = sp, y = mean), data = ag, show.legend = F) +\n  #geom_errorbar(aes(x = sp, ymin = IQ.lower, ymax = IQ.upper, group = sp), data = ag) +\n  geom_boxplot(width = 0.1, show.legend = F) +\n  theme_bw() +\n  labs(title = \"SEAB theta\",\n       x = element_blank(),\n       y = element_blank()) +\n  scale_color_manual(values = global_colors) + \n  theme(aspect.ratio = 1,\n                       panel.grid.major = element_blank(),\n                       panel.grid.minor = element_blank(),\n                       legend.position = c(0.15, 0.1),\n                       legend.background = element_blank()) +\n  facet_wrap(~sp)\n\n\n\n\n\n\n3.4.3 Maturity stages\n\nlbls <- c(\"Adult\", \"Juvenile\")\nage_siber <- custom_siber(full_set, var_col = \"Maturity\", lbls, sp_col = \"Species\", n.burnin = 7000,\n                          colors = global_colors, gp_label = \"Age\")\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 45\n   Unobserved stochastic nodes: 3\n   Total graph size: 60\n\nInitializing model\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 36\n   Unobserved stochastic nodes: 3\n   Total graph size: 51\n\nInitializing model\n\n\nNo id variables; using all as measure variables\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 55\n   Unobserved stochastic nodes: 3\n   Total graph size: 70\n\nInitializing model\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 14\n   Unobserved stochastic nodes: 3\n   Total graph size: 29\n\nInitializing model\n\n\nNo id variables; using all as measure variables\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 48\n   Unobserved stochastic nodes: 3\n   Total graph size: 63\n\nInitializing model\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 26\n   Unobserved stochastic nodes: 3\n   Total graph size: 41\n\nInitializing model\n\n\nNo id variables; using all as measure variables\n\n\n[1] \"All models have been run. Check your working directory for the results\"\n\n\nPlots of ellipses:\n\nage_siber$ellips\n\n\n\n\nEccenctricities:\n\necc <- data.frame()\nfor (sp in seq_along(species)) {\n  for (j in seq_along(lbls)) {\n    ecc <- rbind(ecc,\n               data.frame(sp = species[sp],\n                          j = lbls[j],\n                          ecc = apply(age_siber$ellips[[species[sp]]][[j]][,1:4], 1, mat_ecc),\n                          theta = apply(age_siber$ellips[[species[sp]]][[j]][,1:4], 1, mat_theta)))\n  }\n}\n\n\nec <- ecc %>% group_by(sp, j) %>% summarise(IQR.lower = hdi(ecc, credMass = 0.5)[1],\n                                            IQR.upper = hdi(ecc, credMass = 0.5)[2],\n                                            hdi.lower = hdi(ecc)[1],\n                                            hdi.upper = hdi(ecc)[2],\n                                            mean = mean(ecc))\n\n`summarise()` has grouped output by 'sp'. You can override using the `.groups`\nargument.\n\nec\n\n\n\n  \n\n\n\n\nggplot(data = ecc, aes(x = j, y = ecc, group = j, color = factor(j))) +\n  geom_violin(fill = NA, show.legend = F) +\n  #geom_point(aes(x = sp, y = mean), data = ag, show.legend = F) +\n  #geom_errorbar(aes(x = sp, ymin = IQ.lower, ymax = IQ.upper, group = sp), data = ag) +\n  geom_boxplot(width = 0.1, show.legend = F) +\n  theme_bw() +\n  labs(title = \"SEAB Eccentricity\",\n       x = element_blank(),\n       y = element_blank()) +\n  scale_color_manual(values = global_colors) +\n  theme(aspect.ratio = 1,\n                       panel.grid.major = element_blank(),\n                       panel.grid.minor = element_blank(),\n                       legend.position = c(0.15, 0.1),\n                       legend.background = element_blank()) +\n  facet_wrap(~sp)\n\n\n\n\nTheta:\n\nth <- ecc %>% group_by(sp, j) %>% summarise(IQR.lower = hdi(theta, credMass = 0.5)[1],\n                                            IQR.upper = hdi(theta, credMass = 0.5)[2],\n                                            hdi.lower = hdi(theta)[1],\n                                            hdi.upper = hdi(theta)[2],\n                                            mean = mean(theta))\n\n`summarise()` has grouped output by 'sp'. You can override using the `.groups`\nargument.\n\nth\n\n\n\n  \n\n\n\n\nggplot(data = ecc, aes(x = j, y = theta, group = j, color = factor(j))) +\n  geom_violin(fill = NA, show.legend = F) +\n  #geom_point(aes(x = sp, y = mean), data = ag, show.legend = F) +\n  #geom_errorbar(aes(x = sp, ymin = IQ.lower, ymax = IQ.upper, group = sp), data = ag) +\n  geom_boxplot(width = 0.1, show.legend = F) +\n  theme_bw() +\n  labs(title = \"SEAB theta\",\n       x = element_blank(),\n       y = element_blank()) +\n  scale_color_manual(values = global_colors) + \n  theme(aspect.ratio = 1,\n                       panel.grid.major = element_blank(),\n                       panel.grid.minor = element_blank(),\n                       legend.position = c(0.15, 0.1),\n                       legend.background = element_blank()) +\n  facet_wrap(~sp)\n\n\n\n\n\n\n3.4.4 Season\n\nlbls <- c(\"Warm\", \"Cold\")\nseason_siber <- custom_siber(full_set, var_col = \"Season\", lbls, sp_col = \"Species\",\n                             colors = global_colors, gp_label = \"Season\",\n                             n.burnin = 750000, n.iter = 10000) \n\nToo few points to calculate an ellipse\n\n\nWarning: Removed 1 row containing missing values (`geom_path()`).\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 63\n   Unobserved stochastic nodes: 3\n   Total graph size: 78\n\nInitializing model\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 18\n   Unobserved stochastic nodes: 3\n   Total graph size: 33\n\nInitializing model\n\n\nNo id variables; using all as measure variables\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 15\n   Unobserved stochastic nodes: 3\n   Total graph size: 30\n\nInitializing model\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 54\n   Unobserved stochastic nodes: 3\n   Total graph size: 69\n\nInitializing model\n\n\nNo id variables; using all as measure variables\n\n\nWarning in createSiberObject(subdata[subdata[, sp_col] == species[i], 3:6]): At least one of your groups has less than 5 observations.\n          The absolute minimum sample size for each group is 3 in order\n          for the various ellipses and corresponding metrics to be \n          calculated. More reasonably though, a minimum of 5 data points\n          are required to calculate the two means and the 2x2 covariance \n          matrix and not run out of degrees of freedom. Check the item \n          named 'sample.sizes' in the object returned by this function \n          in order to locate the offending group. Bear in mind that NAs in \n          the sample.size matrix simply indicate groups that are not \n          present in that community, and is an acceptable data structure \n          for these analyses.\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 71\n   Unobserved stochastic nodes: 3\n   Total graph size: 86\n\nInitializing model\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 3\n   Unobserved stochastic nodes: 3\n   Total graph size: 18\n\nInitializing model\n\n\nNo id variables; using all as measure variables\n\n\n[1] \"All models have been run. Check your working directory for the results\"\n\n\nPlots of ellipses:\n\nseason_siber$ellips\n\n\n\n\n\necc <- data.frame()\nfor (sp in seq_along(species)) {\n  for (j in seq_along(lbls)) {\n    ecc <- rbind(ecc,\n               data.frame(sp = species[sp],\n                          j = lbls[j],\n                          ecc = apply(season_siber$ellips[[species[sp]]][[j]][,1:4], 1, mat_ecc),\n                          theta = apply(season_siber$ellips[[species[sp]]][[j]][,1:4], 1, mat_theta)))\n  }\n}\n\n\nec <- ecc %>% group_by(sp, j) %>% summarise(IQR.lower = hdi(ecc, credMass = 0.5)[1],\n                                            IQR.upper = hdi(ecc, credMass = 0.5)[2],\n                                            hdi.lower = hdi(ecc)[1],\n                                            hdi.upper = hdi(ecc)[2],\n                                            mean = mean(ecc))\n\n`summarise()` has grouped output by 'sp'. You can override using the `.groups`\nargument.\n\nec\n\n\n\n  \n\n\n\n\nggplot(data = ecc, aes(x = j, y = ecc, group = j, color = factor(j))) +\n  geom_violin(fill = NA, show.legend = F) +\n  #geom_point(aes(x = sp, y = mean), data = ag, show.legend = F) +\n  #geom_errorbar(aes(x = sp, ymin = IQ.lower, ymax = IQ.upper, group = sp), data = ag) +\n  geom_boxplot(width = 0.1, show.legend = F) +\n  theme_bw() +\n  labs(title = \"SEAB Eccentricity\",\n       x = element_blank(),\n       y = element_blank()) +\n  scale_color_manual(values = global_colors) +\n  theme(aspect.ratio = 1,\n                       panel.grid.major = element_blank(),\n                       panel.grid.minor = element_blank(),\n                       legend.position = c(0.15, 0.1),\n                       legend.background = element_blank()) +\n  facet_wrap(~sp)\n\n\n\n\nTheta:\n\nth <- ecc %>% group_by(sp, j) %>% summarise(IQR.lower = hdi(theta, credMass = 0.5)[1],\n                                            IQR.upper = hdi(theta, credMass = 0.5)[2],\n                                            hdi.lower = hdi(theta)[1],\n                                            hdi.upper = hdi(theta)[2],\n                                            mean = mean(theta))\n\n`summarise()` has grouped output by 'sp'. You can override using the `.groups`\nargument.\n\nth\n\n\n\n  \n\n\n\n\nggplot(data = ecc, aes(x = j, y = theta, group = j, color = factor(j))) +\n  geom_violin(fill = NA, show.legend = F) +\n  #geom_point(aes(x = sp, y = mean), data = ag, show.legend = F) +\n  #geom_errorbar(aes(x = sp, ymin = IQ.lower, ymax = IQ.upper, group = sp), data = ag) +\n  geom_boxplot(width = 0.1, show.legend = F) +\n  theme_bw() +\n  labs(title = \"SEAB theta\",\n       x = element_blank(),\n       y = element_blank()) +\n  scale_color_manual(values = global_colors) + \n  theme(aspect.ratio = 1,\n                       panel.grid.major = element_blank(),\n                       panel.grid.minor = element_blank(),\n                       legend.position = c(0.15, 0.1),\n                       legend.background = element_blank()) +\n  facet_wrap(~sp)\n\n\n\n\n\n\n3.4.5 Final forestplot:\n\nforest_data <- rbind(age_siber[[6]], season_siber[[6]], sex_siber[[6]])\n\nforest <- ggplot(data = forest_data) +\n               geom_vline(xintercept = 0, linetype = \"dashed\", color = \"#C9C8C8\", size = 0.5) +\n               geom_linerange(aes(y = comp, xmin = IQr.lower, xmax = IQr.upper),\n                              color = global_colors[1], size = 1) +\n               geom_linerange(aes(y = comp, xmin = hdi.lower, xmax = hdi.upper),\n                              color = global_colors[1]) +\n               geom_point(aes(x = mean, y = comp),\n                          color = global_colors[1], fill = \"white\", size = 1.5, shape = 21) +\n               theme_bw() +\n               theme(aspect.ratio = 1,\n                     panel.grid.major = element_blank(),\n                     panel.grid.minor = element_blank(),\n                     legend.position = c(0.2, 0.15),\n                     legend.background = element_blank()) +\n               scale_x_continuous(breaks = scales::pretty_breaks(n = 3)) +\n               labs(title = element_blank(),\n                    x = element_blank(),\n                    y = element_blank()) + facet_wrap(~sp)\n\n# cairo_pdf(\"output/SIBER/forest_cats.pdf\", family = \"Times\", width = 4, height = 2)\nforest\n\n\n\n# dev.off()\n\nTabular results:\nSeasons:\n\nseason_siber$diff_glob[,c(\"comp\", \"sp\", \"mean\")]\n\n\n\n  \n\n\nseason_siber$p_sup\n\n\n\n  \n\n\n\nSexes:\n\nsex_siber$diff_glob[,c(\"comp\", \"sp\", \"mean\")]\n\n\n\n  \n\n\nsex_siber$p_sup\n\n\n\n  \n\n\n\nMaturity stages:\n\nage_siber$diff_glob[,c(\"comp\", \"sp\", \"mean\")]\n\n\n\n  \n\n\nage_siber$p_sup"
  },
  {
    "objectID": "SI4_nicheROVER.html#libraries",
    "href": "SI4_nicheROVER.html#libraries",
    "title": "4  Isotopic niche overlaps",
    "section": "4.1 Libraries",
    "text": "4.1 Libraries\n\nlibrary(nicheROVER)\nlibrary(ggplot2)\nlibrary(dplyr)"
  },
  {
    "objectID": "SI4_nicheROVER.html#data",
    "href": "SI4_nicheROVER.html#data",
    "title": "4  Isotopic niche overlaps",
    "section": "4.2 Data",
    "text": "4.2 Data\n\ndata <- read.csv(\"data/glm.csv\", header = T)\ncolnames(data)[6:7] <- c(\"iso1\", \"iso2\")\nglobal <- subset(data, select = c(sp, iso1, iso2))\nglobal$group <- factor(global$sp, labels = c(1:3))\nglobal$community <- 1\nhead(global)"
  },
  {
    "objectID": "SI4_nicheROVER.html#projections-of-niche-regions",
    "href": "SI4_nicheROVER.html#projections-of-niche-regions",
    "title": "4  Isotopic niche overlaps",
    "section": "4.3 Projections of niche regions",
    "text": "4.3 Projections of niche regions\nBasic parameters for plotting:\n\nglobal_colors <- c('#1f77b4', '#ff7f0e', '#2ca02c')\nxlims <- c(min(global$iso2)-0.5, max(global$iso2)+0.5)\nylims <- c(min(global$iso1)-0.5, max(global$iso1)+0.5)\nspecies <- unique(global$sp)\n\nnicheROVER performs a Bayesian estimation of the isotopic niche and then calculates the probability of its overlap with the isotopic niche of another; i.e., how probable is to find an individual of one species in the isotopic space of another. Thus, this overlap is directional and more informative than the traditional, non-directional, geometric approximation (Swanson et al. 2015).\n\n# Number of posterior samples\nnsamples <- 1000\n\n# Estimates of the isotopic niche\nbpar <- tapply(1:nrow(global), global$sp,\n               function(ii) niw.post(nsamples = nsamples,\n                                     X = global[ii, 2:3]))\n\nbdata <- tapply(1:nrow(global),\n                global$sp,\n                function(ii) X = global[ii, 2:3])\n\nPlot of the isotopic niches:\n\nniche.plot(niche.par = bpar,\n           niche.data = bdata,\n           pfrac = 0.05,\n           iso.names = expression(delta^{15}*N, delta^{13}*C),\n           col = global_colors,\n           xlab = expression (\"Isotope ratio (‰)\"))\n\n\n\n\nMean overlaps with the means for 95% and 99% ellipses:\n\nover.stat <- overlap(bpar,\n                     nreps = nsamples,\n                     nprob = 1e3,\n                     alpha = c(0.95, 0.99))\nover.mean <- apply(over.stat,\n                   c(1:2, 4),\n                   mean)*100\nround(over.mean)\n\n, , alpha = 95%\n\n                 Species B\nSpecies A         H.dipterurus N.entemedor R.steindachneri\n  H.dipterurus              NA           9              25\n  N.entemedor               71          NA               0\n  R.steindachneri          100           0              NA\n\n, , alpha = 99%\n\n                 Species B\nSpecies A         H.dipterurus N.entemedor R.steindachneri\n  H.dipterurus              NA          14              35\n  N.entemedor               96          NA               0\n  R.steindachneri          100           0              NA\n\n\n95% Highest density intervals for the overlaps\n\nover.cred <- apply(over.stat*100,\n                   c(1:2, 4),\n                   quantile,\n                   prob = c(.025, .975),\n                   na.rm = TRUE)\nround(over.cred[,,,1]) # display alpha = .95 niche region\n\n, , Species B = H.dipterurus\n\n       Species A\n        H.dipterurus N.entemedor R.steindachneri\n  2.5%            NA          41              98\n  97.5%           NA          96             100\n\n, , Species B = N.entemedor\n\n       Species A\n        H.dipterurus N.entemedor R.steindachneri\n  2.5%             4          NA               0\n  97.5%           15          NA               0\n\n, , Species B = R.steindachneri\n\n       Species A\n        H.dipterurus N.entemedor R.steindachneri\n  2.5%            17           0              NA\n  97.5%           33           0              NA\n\n\nPlot with the posterior distributions of the niche overlaps:\n\nover.stat <- overlap(bpar, nreps = nsamples, nprob = 1e3, alpha = .95)\n#cairo_pdf(\"nicheROVER.pdf\", width = 7, height = 7/1.61, family = \"Times\")\noverlap.plot(over.stat, col = global_colors,\n             mean.cred.col = \"firebrick\",\n             equal.axis = TRUE,\n             xlab = \"Overlap Probability (%)\")\n\n\n\n#dev.off()\n\n\n\n\n\nSwanson, Heidi K., Martin Lysy, Michael Power, Ashley D. Stasko, Jim D. Johnson, and James D. Reist. 2015. “A new probabilistic method for quantifying n dimensional ecological niches and niche overlap.” Ecology 96 (2): 318–24. https://doi.org/https://doi.org/10.1890/14-0235.1."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Armhein, V., S. Greenland, and B. McShane. 2019. “Scientists Rise\nup Against Statistical Significance.” Nature 567 (7748):\n305–7. https://doi.org/10.1038/d41586-019-00857-9.\n\n\nBetancourt, Michael. 2017. “A conceptual\nintroduction to Hamiltonian Monte Carlo.” arXiv\nPreprint 1701.02434.\n\n\nBolstad, W. M. 2004. Introduction to Bayesian Statistics. New\nJersey, USA: Wiley-Interscience.\n\n\nCarvajal, G., M. Maucec, and S. Cullick. 2018. “Components of artificial intelligence and data\nanalytics.” In Intelligent Digital Oil and Gas Fields.\nConcepts, Collaboration, and Right-Time Decisions, by G. Carvajal,\nM. Maucec, and S. Cullick, 101–48. Cambridge, Massachusetts, USA: Gulf\nProfessional Publishing. https://doi.org/https://doi.org/10.1016/B978-0-12-804642-5.00004-9.\n\n\nChawla, N. V., K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer. 2002.\n“SMOTE: Synthetic Minority Over-sampling\nTechnique.” Journal of Artificial Intelligence\nResearch 16: 321–57. https://doi.org/10.1613/jair.953.\n\n\nEllison, A. M. 2004. “Bayesian Inference in Ecology.”\nEcology Letters 7: 509–20. https://doi.org/10.1111/j.1461-0248.2004.00603.x.\n\n\nGabry, Jonah, Daniel Simpson, Aki Vehtari, Michael Betancourt, and\nAndrew Gelman. 2017. “Visualization in\nBayesian workflow.” arXiv Preprint 1709.01449.\nhttps://doi.org/https://doi.org/10.1111/rssa.12378.\n\n\nGerrodette, T. 2011. “Inference Without Significance: Measuring\nSupport for Hypotheses Rather Than Rejecting Them.” Marine\nEcology 32: 404–18. https://doi.org/10.1111/j.1439-0485.2011.00466.x.\n\n\nHobbs, N. T., and R. Hilborn. 2006. “Alternatives to\nStatistical Hypothesis Testing in Ecology: A Guide to Self\nTeaching.” Ecological Applications 16 (1): 5–19.\n\n\nJackson, A. L., R. Inger, A. C. Parnell, and S. Bearhop. 2011.\n“Comparing Isotopic Niche Widths Among and Within Communities:\nSIBER-Stable Isotope Bayesian Ellipses in\nR.” Journal of Animal Ecology 34 (3):\n595–602.\n\n\nKruschke, J. K. 2012. “Bayesian Estimation Supersedes the t\nTest.” Journal of Experimental Psychology: General 142\n(2): 573–603. https://doi.org/https://doi.org/10.1037/a0029146.\n\n\n———. 2015. Doing Bayesian Data Analysis: A Tutorial with r, JAGS,\nand Stan. 2nd ed. London, UK: Academic Press.\n\n\nLewandowski, D., D. Kurowicka, and H. Joe. 2009. “Generating\nRandom Correlation Matrices Based on Vines and Extended Onion\nMethod.” Journal of Multivariate Analysis 100:\n1989–2001. https://doi.org/10.1016/j.jmva.2009.04.008.\n\n\nLink, W. A., and M. J. Eaton. 2012. “On Thinning of Chains in\nMCMC.” Methods in Ecology and Evolution 3: 112–15. https://doi.org/10.1111/j.2041-210X.2011.00131.x.\n\n\nLundberg, Scott M., Gabriel Erion, Hugh Chen, Alex DeGrave, Jordan M.\nPrutkin, Bala Nair, Ronit Katz, Jonathan Himmelfarb, Nisha Bansal, and\nSu-In Lee. 2020. “From Local Explanations to Global Understanding\nwith Explainable AI for Trees.” Nature Machine\nIntelligence 2 (1): 56–67. https://doi.org/10.1038/s42256-019-0138-9.\n\n\nLundberg, Scott, and Su-In Lee. 2017. “A Unified Approach to\nInterpreting Model Predictions.” arXiv Preprint\n1705.07874.\n\n\nMartin, O. 2018. Bayesian Analysis with Python: Introduction to\nStatistical Modeling and Probabilistic Programming Using PyMC3 and\nArviZ. 2nd ed. Pakt Publishing.\n\n\nMeyer-Baese, A., and V. Schmid. 2014. “Chapter 7 -\nFoundations of Neural Networks.” In Pattern\nRecognition and Signal Analysis in Medical Imaging, edited by A.\nMeyer-Baese and V. Schmid, 2nd ed., 197–243. Oxford: Academic Press.\nhttps://doi.org/https://doi.org/10.1016/B978-0-12-409545-8.00007-8.\n\n\nOshiro, T. M., P. S. Perez, and J. A. Baranauskas. 2012. “How Many\nTrees in a Random Forest?” In Machine Learning and Data\nMining in Pattern Recognition, edited by P. Perner, 7376:154–68.\nSpringer Berlin Heidelberg. https://doi.org/10.1007/978-3-642-31537-4_13.\n\n\nPark, T., and G. Casella. 2012. “The Bayesian\nLasso.” Journal of the American Statistical\nAssociation 103 (482): 681–86. https://doi.org/https://doi.org/10.1198/016214508000000337.\n\n\nSalvatier, J., T. V. Wiecki, and C. Fonnesbeck. 2016. “Probabilistic programming in Python using\nPyMC3.” PeerJ Computer Science 2: e55.\nhttps://doi.org/https://doi.org/10.7717/peerj-cs.55.\n\n\nSwanson, Heidi K., Martin Lysy, Michael Power, Ashley D. Stasko, Jim D.\nJohnson, and James D. Reist. 2015. “A new\nprobabilistic method for quantifying n dimensional ecological niches and\nniche overlap.” Ecology 96 (2): 318–24.\nhttps://doi.org/https://doi.org/10.1890/14-0235.1.\n\n\nTeam, Stan Development. 2022. “Stan Modeling Language User’s Guide\nand Reference Manual.” 2022. https://mc-stan.org/docs/reference-manual/effective-sample-size.html.\n\n\nVan Rossum, G., and F. L. Drake. 2009. Python 3 Reference\nManual. Scotts Valley, CA: CreateSpace."
  }
]